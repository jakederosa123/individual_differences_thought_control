{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# adding Folder_2 to the system path\n",
    "sys.path.insert(0, '/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/leiden/analysis/')\n",
    "from clearmem_my_functions import *\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/jade6100/.local/lib/python3.7/site-packages')\n",
    "import scikit_posthocs as sp\n",
    "import itertools\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "import sys \n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "from sklearn.manifold import MDS\n",
    "import scipy.spatial.distance as sp_distance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import animation\n",
    "\n",
    "#import statsmodels.api as sm\n",
    "#from statsmodels.formula.api import ols\n",
    "#from scipy import stats\n",
    "#import scikit_posthocs as sp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import manifold\n",
    "\n",
    "def getF(data, var, group):\n",
    "    data[var] = data[var].astype('float')\n",
    "    model = ols(var + '~ C('+group+')', data=data).fit()\n",
    "    anova_table = np.array(sm.stats.anova_lm(model, type=2)[['F', 'PR(>F)']])[0]\n",
    "    return anova_table.round(4)\n",
    "\n",
    "def getposthoc(data, var, group):\n",
    "    data[var] = data[var].astype('float')\n",
    "    model = ols(var + '~ C('+group+')', data=data).fit()\n",
    "    post_hoc = sp.posthoc_ttest(data, val_col=var, group_col=group,\n",
    "                                p_adjust='fdr_bh').sort_index().sort_index(axis = 1)\n",
    "    post_hoc.columns = list(range(1, len(uni(data[group]))+1))\n",
    "    return post_hoc\n",
    "\n",
    "def vsim(data, parcel=None):\n",
    "    \n",
    "    if parcel is None:\n",
    "        parcel_row = pd.DataFrame(np.flip(np.array(data.T))).T\n",
    "        \n",
    "    else:\n",
    "        parcel_row = pd.DataFrame(np.flip(np.array(data.iloc[[parcel]].T))).T\n",
    "    a = np.array(parcel_row.T.iloc[:, 0: 1]).squeeze()\n",
    "    n = int(np.sqrt(len(a)*2))+1\n",
    "    mask = np.tri(n,dtype=bool, k=-1) # or np.arange(n)[:,None] > np.arange(n)\n",
    "    out = np.zeros((n,n))\n",
    "    out[mask] = a\n",
    "    out = out + out.T - np.diag(np.diag(out))\n",
    "    out[np.diag_indices_from(out)]\n",
    "    sim_mat = pd.DataFrame(np.flip(out))\n",
    "  \n",
    "\n",
    "    return(sim_mat)\n",
    "    \n",
    "def grad_centroid_vsim(data, grad): \n",
    "    new_row_cluster = data[data['grad'] == grad]\n",
    "    new_row_cluster = new_row_cluster.drop(['grad'], axis=1)\n",
    "    clust_centroid = pd.DataFrame(new_row_cluster.mean(), columns = ['Mean']).T\n",
    "    \n",
    "    sim_mat = vsim(clust_centroid)\n",
    "    \n",
    "    return(sim_mat)\n",
    "\n",
    "\n",
    "def threshold_proportional(W,p,copy=True):\n",
    "\n",
    "    if p>1 or p<0:\n",
    "        raise BCTParamError('Threshold must be in range [0,1]')\n",
    "    if copy: W=W.copy()\n",
    "    n=len(W)\t\t\t\t\t\t# number of nodes\n",
    "    np.fill_diagonal(W, 0)\t\t\t# clear diagonal\n",
    "\n",
    "    if np.all(W==W.T):\t\t\t\t# if symmetric matrix\n",
    "        W[np.tril_indices(n)]=0\t\t# ensure symmetry is preserved\n",
    "        ud=2\t\t\t\t\t\t# halve number of removed links\n",
    "    else:\n",
    "        ud=1\n",
    "\n",
    "    ind=np.where(W)\t\t\t\t\t# find all links\n",
    "\n",
    "    I=np.argsort(W[ind])[::-1]\t\t# sort indices by magnitude\n",
    "\n",
    "    en=int(round((n*n-n)*p/ud))\t\t# number of links to be preserved\n",
    "\n",
    "    W[(ind[0][I][en:],ind[1][I][en:])]=0\t# apply threshold\n",
    "    #W[np.ix_(ind[0][I][en:], ind[1][I][en:])]=0\n",
    "\n",
    "    if ud==2:\t\t\t\t\t\t# if symmetric matrix\n",
    "        W[:,:]=W+W.T\t\t\t\t\t\t# reconstruct symmetry\n",
    "\n",
    "    return W\n",
    "\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import itertools\n",
    "\n",
    "def plot_3Dfigure(newX, colors, title='', net=None, filepath=None):\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    if net is not None:\n",
    "        data = go.Scatter3d(x=newX[:,0], y=newX[:,1], z=newX[:,2], \n",
    "                            mode='markers',\n",
    "                            marker=dict(size=5,\n",
    "                                        #color=newX[:,1],\n",
    "                                        color=colors,\n",
    "                                        opacity=0.7,\n",
    "                                        colorscale=colors)\n",
    "                           )\n",
    "    else:\n",
    "        data = go.Scatter3d(x=newX[:,0], y=newX[:,1], z=newX[:,2], \n",
    "                            mode='markers',\n",
    "                            marker=dict(size=5,\n",
    "                                        color=newX[:,1],\n",
    "                                        opacity=0.7,\n",
    "                                        colorscale='jet')\n",
    "                           )\n",
    "    \n",
    "    layout = go.Layout(title_text=title,title_x=0.5,title_y=0.8,title_font_size=12)\n",
    "    fig = go.Figure(data=[data], layout=layout)\n",
    "    fig.update_layout(showlegend=False)\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(showticklabels=False)\n",
    "    fig.update_layout(scene = dict(\n",
    "                    xaxis = dict(title= '', ticks= '', showticklabels= False,),\n",
    "                    yaxis = dict(title= '', ticks= '', showticklabels= False,),\n",
    "                    zaxis = dict(title= '', ticks= '', showticklabels= False,),\n",
    "                    ))\n",
    "    fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "    #fig.show()\n",
    "    \n",
    "    if filepath is not None:\n",
    "        fig.write_html(filepath+'3d_grads.html')\n",
    "        \n",
    "\n",
    "from pylab import *\n",
    "\n",
    "def get_color_maps(cmap, n, nrep):\n",
    "    from PIL import Image, ImageColor\n",
    "\n",
    "    cmap = cm.get_cmap(cmap, n)    # PiYG\n",
    "    converted_list = []\n",
    "    for i in range(cmap.N):\n",
    "        rgba = cmap(i)\n",
    "        # rgb2hex accepts rgb or rgba\n",
    "        hexc = matplotlib.colors.rgb2hex(rgba)\n",
    "        codes = ImageColor.getcolor(str(hexc), \"RGB\")\n",
    "\n",
    "        c1 = list(itertools.repeat(int(codes[0])/255, nrep))\n",
    "        c2 = list(itertools.repeat(int(codes[1])/255, nrep))\n",
    "        c3 = list(itertools.repeat(int(codes[2])/255, nrep))\n",
    "        \n",
    "        converted = pd.concat([pd.DataFrame(c1), pd.DataFrame(c2), pd.DataFrame(c3)], axis = 1)\n",
    "        converted.columns = ['r', 'g', 'b']\n",
    "        converted_list.append(converted)\n",
    "        \n",
    "    final = pd.concat(converted_list).reset_index(drop=True)\n",
    "    final['index_new'] = list(range(0,final.shape[0]))\n",
    "    \n",
    "    return final\n",
    "\n",
    "def show_grads(data):\n",
    "    import nibabel as nib\n",
    "    import nilearn.plotting as plotting\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "    import pandas as pd\n",
    "    import hcp_utils as hcp\n",
    "\n",
    "    def listToDict(lstA, lstB):\n",
    "        zipped = zip(lstA, lstB)\n",
    "        op = dict(zipped)\n",
    "        return op\n",
    "\n",
    "    def uni(list1):\n",
    "        unique_list = []\n",
    "        # traverse for all elements\n",
    "        for x in list1:\n",
    "            # check if exists in unique_list or not\n",
    "            if x not in unique_list:\n",
    "                unique_list.append(x)\n",
    "\n",
    "        return(unique_list)\n",
    "\n",
    "    glasser = pd.read_csv('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/leiden/analysis/glasser_regions/spearman_subtype_glasser_regions.csv').drop('regionID', axis =1).rename({'Unnamed: 0':'regionID'}, axis = 1)\n",
    "    glasser['regionID'] = glasser['regionID'] + 1\n",
    "    \n",
    "    mesh_path = '/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/leiden/hcp-utils/hcp_utils/data/'\n",
    "    mesh_sub = hcp.load_surfaces(example_filename=mesh_path+'S1200.R.pial_MSMAll.32k_fs_LR.surf.gii')\n",
    "    \n",
    "    regs = data \n",
    "    regs['regionID'] = regs['index'] + 1\n",
    "    regs = pd.merge(regs, glasser, on = 'regionID')\n",
    "    regs = regs[['regionID', 'Subtype_x', 'grad', 'regionName', 'regionLongName', 'Lobe', 'cortex', 'r', 'g', 'b']]\n",
    "  \n",
    "    maps_df = pd.DataFrame(hcp.mmp.rgba).T\n",
    "    maps_df = maps_df.reset_index().rename({'index':'regionID'}, axis = 1)\n",
    "    maps_df['regionID'] = maps_df['regionID'] +1\n",
    "    maps_df = pd.DataFrame(hcp.mmp.rgba).T\n",
    "    maps_df = maps_df.reset_index().rename({'index':'regionID'}, axis = 1)\n",
    "    maps_df['regionID'] = maps_df['regionID'] +1\n",
    "\n",
    "    new_maps_df = pd.merge(regs, maps_df, on = \"regionID\").sort_values('regionID')\n",
    "    new_maps_df[0] = new_maps_df['r']\n",
    "    new_maps_df[1] = new_maps_df['g']\n",
    "    new_maps_df[2] = new_maps_df['b']\n",
    "    new_maps_df = new_maps_df[[0, 1,2,3]]\n",
    "\n",
    "    first = pd.DataFrame(np.array([0, 0, 0, 0])).T\n",
    "    more = pd.concat([pd.DataFrame(np.zeros(20)), pd.DataFrame(np.zeros(20)), pd.DataFrame(np.zeros(20)), pd.DataFrame(np.zeros(20))], axis= 1)\n",
    "    more.columns = [0,1,2,3]\n",
    "\n",
    "    new_maps_df = pd.concat([first, new_maps_df, more])\n",
    "\n",
    "    map_l = []\n",
    "    out_l = []\n",
    "    for i in range(0, len(new_maps_df)):\n",
    "        map_l.append(i)\n",
    "        out_l.append(np.array(new_maps_df.iloc[i:i+1])[0])\n",
    "\n",
    "    hcp.mmp.rgba = listToDict(map_l, out_l)\n",
    "\n",
    "    return hcp.view_parcellation(mesh_sub.inflated, hcp.mmp)\n",
    "\n",
    "def get_colors(cmap, n):\n",
    "        from PIL import Image, ImageColor\n",
    "\n",
    "        cmap = cm.get_cmap(cmap, n)    # PiYG\n",
    "        converted_list = []\n",
    "        for i in range(cmap.N):\n",
    "            rgba = cmap(i)\n",
    "            # rgb2hex accepts rgb or rgba\n",
    "            hexc = matplotlib.colors.rgb2hex(rgba)\n",
    "            converted_list.append(hexc)\n",
    "\n",
    "        return converted_list\n",
    "    \n",
    "    \n",
    "def show_code(function):  \n",
    "    import inspect\n",
    "    lines = inspect.getsource(function)\n",
    "    print(lines)\n",
    "    \n",
    "jets = get_colors('jet_r', 20)\n",
    "\n",
    "\n",
    "net_cols = ['#FCFF0D', '#21DFB4', '#4E00A2', '#F00087']\n",
    "ops_cols = ['#F0180A', '#F08B0A', '#6DAE45', '#0A5AF0']\n",
    "\n",
    "\n",
    "\n",
    "def reduce_memory_usage(df, verbose=False):\n",
    "    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from glob import glob\n",
    "subj_ids = '/pl/active/banich/studies/wmem/fmri/operation_rsa/subj/*'\n",
    "subj_ids_sorted = sorted(glob(subj_ids, recursive = True))\n",
    "\n",
    "for i in range(0,len(subj_ids_sorted)):\n",
    "    subj_ids_sorted[i] = subj_ids_sorted[i].replace('/pl/active/banich/studies/wmem/fmri/operation_rsa/subj/subj_operation_sub-', '').replace('/', '').replace('.mat', '')\n",
    "    #subj_ids_sorted[i] = 'sub' + subj_ids_sorted[i] + \"-\" + str(i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_glob_path = '/pl/active/banich/studies/wmem/fmri/mvpa/utaustin/data/clearmem_v1_*/mvpa_operation/parse_sh10_4/table_target_evidence_operation_highres001_brain_grey_3mm_bin0.2_bold_mcf_brain_hpass_dt_shift10tr_norest_fixpen50.csv'\n",
    "accuracy_glob_path = '/pl/active/banich/studies/wmem/fmri/mvpa/utaustin/data/clearmem_v1_*/mvpa_operation/parse_sh10_4/table_target_accuracy_operation_highres001_brain_grey_3mm_bin0.2_bold_mcf_brain_hpass_dt_shift10tr_norest_fixpen50.csv'\n",
    "from glob import glob\n",
    "\n",
    "evidence_sub_idx = sorted(glob(evidence_glob_path, recursive = True))\n",
    "accuracy_sub_idx = sorted(glob(accuracy_glob_path, recursive = True))\n",
    "\n",
    "for i in range(0,len(evidence_sub_idx)):\n",
    "    evidence_sub_idx[i] = evidence_sub_idx[i].replace('/pl/active/banich/studies/wmem/fmri/mvpa/utaustin/data/clearmem_v1_', '').replace('sub', '')\n",
    "    evidence_sub_idx[i] = evidence_sub_idx[i].replace('/mvpa_operation/parse_sh10_4/table_target_evidence_operation_highres001_brain_grey_3mm_bin0.2_bold_mcf_brain_hpass_dt_shift10tr_norest_fixpen50.csv', '')\n",
    "\n",
    "for i in range(0,len(accuracy_sub_idx )):\n",
    "    accuracy_sub_idx[i] = accuracy_sub_idx[i].replace('/pl/active/banich/studies/wmem/fmri/mvpa/utaustin/data/clearmem_v1_', '').replace('sub', '')\n",
    "    accuracy_sub_idx[i] = accuracy_sub_idx[i].replace('/mvpa_operation/parse_sh10_4/table_target_accuracy_operation_highres001_brain_grey_3mm_bin0.2_bold_mcf_brain_hpass_dt_shift10tr_norest_fixpen50.csv', '')\n",
    "\n",
    "evidence_sub_ids_df = pd.DataFrame(evidence_sub_idx).rename({0:'ID'}, axis=1)\n",
    "accruacy_sub_ids_df = pd.DataFrame(accuracy_sub_idx).rename({0:'ID'}, axis=1)\n",
    "\n",
    "ev_acc_ids_df = pd.merge(evidence_sub_ids_df, accruacy_sub_ids_df, on ='ID')\n",
    "\n",
    "sub_ids_df = pd.DataFrame(subj_ids_sorted).rename({0:'ID'}, axis=1)\n",
    "sub_ids_df['matched_id'] = list(range(1,56,1))\n",
    " \n",
    "matched_ids_df = pd.merge(ev_acc_ids_df , sub_ids_df, on =\"ID\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_zero_list =[]\n",
    "for i in list(matched_ids_df['matched_id']):\n",
    "    \n",
    "    if len(str(i)) == 1:\n",
    "        x = \"00\" + str(i)\n",
    "    else: \n",
    "        x = \"0\" + str(i)\n",
    "    \n",
    "    add_zero_list.append(x)\n",
    "    \n",
    "matched_ids_df['matched_id'] = add_zero_list\n",
    "\n",
    "accuracy_list = []\n",
    "evidence_list = []\n",
    "for i,j in zip(sorted(glob(accuracy_glob_path, recursive = True)),\n",
    "               sorted(glob(evidence_glob_path, recursive = True))):\n",
    "               accuracy_list.append(pd.read_csv(i)[['subject_id', 't1_maintain', 't2_repCat', 't3_target', 't4_global', 'total_accuracy']])\n",
    "               evidence_list.append(pd.read_csv(j))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_func_mat(data):\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    X = np.array(data)\n",
    "    \n",
    "    X_Z = (X - np.mean(X, axis=0, keepdims=True)) / np.std(X, axis=0, dtype=np.float64, ddof=1, keepdims=True)\n",
    "    D, rho = spearmanr(np.array(X_Z), axis=1)\n",
    "\n",
    "    perc = np.array([np.percentile(x, 90) for x in D])\n",
    "\n",
    "    for i in range(D.shape[0]):\n",
    "        D[i, D[i,:] < perc[i]] = 0    \n",
    "    \n",
    "    D[D < 0] = 0\n",
    "    #D = 1 - pairwise_distances(D, metric = 'cosine')\n",
    "    #scaler = MinMaxScaler(feature_range=(.5,1))\n",
    "\n",
    "    #D = scaler.fit_transform(D)\n",
    "    \n",
    "    D = pd.DataFrame(D)\n",
    "    # D = aff\n",
    "    \n",
    "    return(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = pd.read_csv('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/leiden/group_sm_vector_spearman/Output/Results/group_sm_vector_spearman_Full_Subtypes.csv')\n",
    "\n",
    "gmat_main = pd.DataFrame(create_func_mat(group.iloc[:, 5:].iloc[:, 0:10332]))\n",
    "gmat_replace = pd.DataFrame(create_func_mat(group.iloc[:, 5:].iloc[:, 10332:int(10332*2)]))\n",
    "gmat_suppress = pd.DataFrame(create_func_mat(group.iloc[:, 5:].iloc[:, int(10332*2):int(10332*3)]))\n",
    "gmat_clear = pd.DataFrame(create_func_mat(group.iloc[:, 5:].iloc[:, int(10332*3):int(10332*4)]))\n",
    "\n",
    "group_subs = group[['Unnamed: 0', 'Subtype']].rename({'Unnamed: 0':'index'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_path = '/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/leiden/subj_sm_ind/*'\n",
    "from glob import glob\n",
    "\n",
    "sub_idx = glob(glob_path, recursive = True)\n",
    "\n",
    "for i in range(0,len(sub_idx)):\n",
    "    sub_idx[i] = sub_idx[i].replace('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/leiden/subj_sm_ind/', '').replace('/', '')\n",
    "    \n",
    "sub_idx = list(filter(lambda k: 'sub' in k, sub_idx))\n",
    "\n",
    "sub_idx = list(filter(lambda k: '.csv' not in k, sub_idx))\n",
    "\n",
    "#remove sub019 because no accuracy or evdidence classifiers\n",
    "sub_idx = sorted([x for x in sub_idx if 'sub019_sm_vector' not in x])\n",
    "sub_idx = sub_idx[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_mats(data, parcel_index, gfm):\n",
    "    \n",
    "    sfm = pd.DataFrame(create_func_mat(data))\n",
    "    sfm.columns = parcel_index\n",
    "    sfm.index = parcel_index\n",
    "    smf_cols = list(pd.DataFrame(sfm).columns)\n",
    "\n",
    "    not_in_list = [x for x in list(range(360)) if x not in smf_cols]\n",
    "    \n",
    "    sfm_copy = sfm.copy()\n",
    "    for i in not_in_list:\n",
    "        sfm_copy[i] = gfm[i]\n",
    "        \n",
    "    sfm_copy = sfm_copy.T\n",
    "\n",
    "    for i in not_in_list:\n",
    "        sfm_copy[i] = gfm[i]\n",
    "\n",
    "    sfm_copy = pd.DataFrame(sfm_copy.sort_index().sort_index(axis=1))\n",
    "\n",
    "\n",
    "    return(sfm_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mats(data_path, group_subs):\n",
    "    \n",
    "    df1 = pd.read_csv(data_path).drop('Subtype', axis =1)\n",
    "\n",
    "    main_cols = list(map(str,range(0, 10332)))\n",
    "    replace_cols = list(map(str,range(10332, 10332*2)))\n",
    "    suppress_cols = list(map(str,range(10332*2, 10332*3)))\n",
    "    clear_cols = list(map(str,range(10332*3, 10332*4)))\n",
    "    \n",
    "    lh = list(df1[df1['node'].str.contains(\"LH\")]['node'].str.replace(r'\\D', '').astype(int) -1)\n",
    "    rh = list(df1[df1['node'].str.contains(\"RH\")]['node'].str.replace(r'\\D', '').astype(int) + 179)\n",
    "\n",
    "    parcel_index = lh + rh\n",
    "    df1.index = parcel_index\n",
    "    \n",
    "    not_in_list = [x for x in list(range(360)) if x not in parcel_index]\n",
    "    \n",
    "    df1_cols = list(df1.iloc[:, 4:].columns)\n",
    "    \n",
    "    df1['index'] = df1.index\n",
    "    df1 = pd.merge(df1, group_subs, on = 'index')\n",
    "    df1.index = parcel_index\n",
    "    \n",
    "    main_cols_pull = [x for x in df1_cols if x in main_cols]\n",
    "    replace_cols_pull = [x for x in df1_cols if x in replace_cols]\n",
    "    suppress_cols_pull = [x for x in df1_cols if x in suppress_cols]\n",
    "    clear_cols_pull = [x for x in df1_cols if x in clear_cols]\n",
    "    \n",
    "    main = df1[main_cols_pull]\n",
    "    replace = df1[replace_cols_pull]\n",
    "    suppress = df1[suppress_cols_pull]\n",
    "    clear = df1[clear_cols_pull]\n",
    "    \n",
    "    main.name = 'main'\n",
    "    replace.name = 'replace'\n",
    "    suppress.name = 'suppress'\n",
    "    clear.name = 'clear'\n",
    "    \n",
    "    if len(parcel_index) < 360:\n",
    "        main_mat = get_new_mats(main, parcel_index, gmat_main)\n",
    "        replace_mat = get_new_mats(replace, parcel_index, gmat_replace)\n",
    "        suppress_mat = get_new_mats(suppress, parcel_index, gmat_suppress)\n",
    "        clear_mat = get_new_mats(clear, parcel_index, gmat_clear)\n",
    "         \n",
    "        ts1_subs = df1[['index', 'Subtype']]\n",
    "        add_subs = group_subs.T[not_in_list].T\n",
    "        copy_frame = df1\n",
    "        ts1_added_subs = pd.concat([ts1_subs, add_subs])\n",
    "        ts1_added_subs = pd.concat([copy_frame.drop(['index', 'Subtype'], axis=1), ts1_added_subs], axis=1)\n",
    "\n",
    "        df1 = ts1_added_subs\n",
    "        \n",
    "    else: \n",
    "        main_mat = create_func_mat(main)\n",
    "        replace_mat = create_func_mat(replace)\n",
    "        suppress_mat = create_func_mat(suppress)\n",
    "        clear_mat = create_func_mat(clear)\n",
    "                                           \n",
    "     \n",
    "    main_mat = reduce_memory_usage(main_mat)\n",
    "    replace_mat = reduce_memory_usage(replace_mat)\n",
    "    suppress_mat = reduce_memory_usage(suppress_mat)\n",
    "    clear_mat = reduce_memory_usage(clear_mat)\n",
    "        \n",
    "\n",
    "    colors = df1.Subtype.map({1:net_cols[0], 2:net_cols[1], 3:net_cols[2], 4:net_cols[3]})\n",
    "    \n",
    "\n",
    "    return df1, parcel_index, not_in_list, colors, main, replace, suppress, clear, main_mat, replace_mat,suppress_mat, clear_mat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class subj_outputs:\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.df1, self.parcel_index, self.not_in_list, self.colors, self.main, self.replace, self.suppress, self.clear, self.main_mat, self.replace_mat, self.suppress_mat, self.clear_mat = get_mats(self.path, group_subs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_path = []\n",
    "for i in sub_idx:\n",
    "    subj_path.append('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/leiden/subj_sm_ind/'+i+'/Output/Results/'+i+'_Full_Subtypes.csv')\n",
    "\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "sub_class_list = *map(subj_outputs, subj_path),\n",
    "end = timer()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_reads = '''subj_path = []\n",
    "for i in sub_idx:\n",
    "    subj_path.append('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/leiden/subj_sm_ind/'+i+'/Output/Results/'+i+'_Full_Subtypes.csv')\n",
    "\n",
    "subj_path = subj_path[0:10]\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "\n",
    "sub_class_list = []\n",
    "for i in subj_path:\n",
    "\n",
    "    class subj_outputs:\n",
    "\n",
    "        subj_path = i\n",
    "\n",
    "        def __init__(self, subj_path):\n",
    "\n",
    "            #print(self.subj_path)\n",
    "\n",
    "            self.df1, self.parcel_index, self.not_in_list, self.colors, self.main, self.replace, self.suppress, self.clear, self.main_mat, self.replace_mat, self.suppress_mat, self.clear_mat = get_mats(self.subj_path, group_subs)\n",
    "\n",
    "    sub_class_list.append(subj_outputs(i))\n",
    "\n",
    "\n",
    "end = timer()\n",
    "print(end - start)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in zip(range(len(sub_class_list)), sub_idx):\n",
    "        print('----------------------- processed:'+j)\n",
    "        os.system(f'mkdir -p /pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/subj/{j}')\n",
    "        sub_class_list[i].name = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path = '/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/subj/'\n",
    "for i in range(len(sub_class_list)):\n",
    "    sub_class_list[i].plot_path = plot_path + sub_class_list[i].name+'/'+sub_class_list[i].name+'_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_all_mats ='''all_mats = []\n",
    "for i in range(len(sub_class_list)):\n",
    "    main_mat_list = sub_class_list[i].main_mat\n",
    "    replace_mat_list =  sub_class_list[i].replace_mat\n",
    "    suppress_mat_list = sub_class_list[i].suppress_mat\n",
    "    clear_mat_list = sub_class_list[i].clear_mat \n",
    "    \n",
    "    all_mats.append(main_mat_list)\n",
    "    all_mats.append(replace_mat_list)\n",
    "    all_mats.append(suppress_mat_list)\n",
    "    all_mats.append(clear_mat_list)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grads(mat_list):\n",
    "    \n",
    "    from brainspace.gradient import GradientMaps\n",
    "    from brainspace.plotting import plot_hemispheres\n",
    "    from brainspace.utils.parcellation import map_to_labels\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    from brainspace.null_models import SurrogateMaps\n",
    "    import seaborn as sns\n",
    "    \n",
    "    gj = GradientMaps(approach='dm',\n",
    "                      kernel='normalized_angle',\n",
    "                      alignment='joint',\n",
    "                      random_state=1)\n",
    "    \n",
    "\n",
    "    global group_grads\n",
    "    gj_out = gj.fit(mat_list)\n",
    "    \n",
    "\n",
    "    for i in range(1):\n",
    "        sns.set_context(\"paper\", font_scale = 2)\n",
    "        \n",
    "        fig, ax = plt.subplots(1, figsize=(5, 4))\n",
    "        ax.scatter(range(gj_out.lambdas_[i].size), gj_out.lambdas_[i])\n",
    "        ax.set_xlabel('Component')\n",
    "        ax.set_ylabel('Eigenvalue')\n",
    "        fig.tight_layout()\n",
    "        fig.savefig('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/grp/eigen_components.png')\n",
    "  \n",
    "        fig, ax = plt.subplots(1, figsize=(5, 4))\n",
    "        variance = gj_out.lambdas_[0]\n",
    "        ax.scatter(range(1,11), variance/variance.sum())\n",
    "        start, end = ax.get_ylim()\n",
    "        ax.yaxis.set_ticks(np.arange(start, end+.05, 0.05))\n",
    "        ax.set_xlabel('Component')\n",
    "        ax.set_ylabel('variance %')\n",
    "        variance/variance.sum()\n",
    "        fig.tight_layout()\n",
    "        fig.savefig('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/grp/variance_explained.png')\n",
    "    \n",
    "    return gj_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmats = [gmat_main, gmat_replace, gmat_suppress, gmat_clear]\n",
    "group_grads = get_grads(gmats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    group_grads.aligned_[i][:,1] = group_grads.aligned_[i][:,1]*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_grp_grads = pd.DataFrame(group_grads.aligned_[0]).iloc[:,0:3]\n",
    "main_grp_grads.columns =['g1', 'g2', 'g3']\n",
    "main_grp_grads.to_csv('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/grp/main_grads.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_grp_grads = pd.DataFrame(group_grads.aligned_[1]).iloc[:,0:3]\n",
    "replace_grp_grads.columns =['g1', 'g2', 'g3']\n",
    "replace_grp_grads.to_csv('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/grp/replace_grads.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppress_grp_grads = pd.DataFrame(group_grads.aligned_[2]).iloc[:,0:3]\n",
    "suppress_grp_grads.columns =['g1', 'g2', 'g3']\n",
    "suppress_grp_grads.to_csv('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/grp/suppress_grads.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_grp_grads = pd.DataFrame(group_grads.aligned_[3]).iloc[:,0:3]\n",
    "clear_grp_grads.columns =['g1', 'g2', 'g3']\n",
    "clear_grp_grads.to_csv('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/grp/clear_grads.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub_class_list = sub_class_list_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brainspace.gradient import GradientMaps\n",
    "from brainspace.plotting import plot_hemispheres\n",
    "from brainspace.utils.parcellation import map_to_labels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from brainspace.null_models import SurrogateMaps\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def align_subs(data):\n",
    "    \n",
    "    global group_grads\n",
    "    \n",
    "    main_mat_list = data.main_mat\n",
    "    replace_mat_list =  data.replace_mat\n",
    "    suppress_mat_list = data.suppress_mat\n",
    "    clear_mat_list = data.clear_mat \n",
    "    \n",
    "    gj = GradientMaps(approach='dm', kernel='normalized_angle', alignment='procrustes', random_state=1)\n",
    "    gj_out_main = gj.fit(np.array(main_mat_list), reference = group_grads.aligned_[0])\n",
    "    print('finished main alignment for: ' +  data.name)\n",
    "    \n",
    "    gj = GradientMaps(approach='dm', kernel='normalized_angle', alignment='procrustes', random_state=1)\n",
    "    gj_out_replace = gj.fit(np.array(replace_mat_list), reference =group_grads.aligned_[1])\n",
    "    print('finished replace alignment for: ' +  data.name)\n",
    "    \n",
    "    gj = GradientMaps(approach='dm', kernel='normalized_angle', alignment='procrustes', random_state=1)\n",
    "    gj_out_suppress = gj.fit(np.array(suppress_mat_list), reference = group_grads.aligned_[2])\n",
    "    print('finished suppress alignment for: ' +  data.name)\n",
    "    \n",
    "    gj = GradientMaps(approach='dm', kernel='normalized_angle', alignment='procrustes', random_state=1)\n",
    "    gj_out_clear = gj.fit(np.array(clear_mat_list), reference = group_grads.aligned_[3])\n",
    "    print('finished clear alignment for: ' +  data.name)\n",
    "        \n",
    "    data.main_aligned = gj_out_main.aligned_\n",
    "    data.replace_aligned = gj_out_replace.aligned_\n",
    "    data.suppress_aligned = gj_out_suppress.aligned_\n",
    "    data.clear_aligned = gj_out_clear.aligned_\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "sub_class_list = *map(align_subs, sub_class_list),\n",
    "end = timer()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_grads(sub_class_list, op):\n",
    "    \n",
    "    if op == 'main':\n",
    "        df =sub_class_list.main_aligned\n",
    "        \n",
    "    elif op == 'replace':\n",
    "        df =sub_class_list.replace_aligned \n",
    "        \n",
    "    elif op == 'suppress':\n",
    "        df =sub_class_list.suppress_aligned\n",
    "        \n",
    "    elif op == 'clear':\n",
    "        df =sub_class_list.clear_aligned\n",
    "        \n",
    "    op_mat = sub_class_list.df1\n",
    "    \n",
    "    filepath = sub_class_list.plot_path+'_'+op+'_'\n",
    "    \n",
    "    rc_g1=df[:,0]\n",
    "    rc_g2=df[:,1]\n",
    "    rc_g3=df[:,2]\n",
    "    rc_g4=df[:,3]\n",
    "    rc_g5=df[:,4]\n",
    "    \n",
    "    Y = np.stack((rc_g1, rc_g2, rc_g3, rc_g4, rc_g5)).T\n",
    "\n",
    "    fig = plt.figure(figsize=(8.5,5.5))\n",
    "  \n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    colors = sub_class_list.colors\n",
    "\n",
    "    ax1.scatter(Y[:, 0], Y[:, 1], c=Y[:, 1], cmap='jet_r')\n",
    "    ax2.scatter(Y[:, 0], Y[:, 1], c=colors)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f'{filepath}_grad_axes.png')\n",
    "\n",
    "    plt.clf()\n",
    "  \n",
    "    #get_anim(Y)\n",
    "    plot_3Dfigure(Y, colors, net=True, filepath=sub_class_list.plot_path+'network_')\n",
    "    plot_3Dfigure(Y, colors, filepath=sub_class_list.plot_path)\n",
    "    \n",
    "    grads = pd.DataFrame(Y)\n",
    "    #grads.index = parcel_index\n",
    "    grads.columns = ['g1', 'g2', 'g3', 'g4', 'g5']\n",
    "    df_grads = pd.concat([op_mat, grads], axis = 1)\n",
    "    sub_grads = df_grads[['Subtype', 'g1', 'g2', 'g3', 'g4', 'g5']].reset_index()\n",
    "    #sub_grads.index = parcel_index\n",
    "\n",
    "   \n",
    "    N = int(op_mat.shape[0])\n",
    "    \n",
    "    if N == 360:\n",
    "        grad_nums = []\n",
    "        for i in range(1,21):\n",
    "            grad_nums.append(list(itertools.repeat(i, int(360/20))))\n",
    "            #grad_nums = list(itertools.chain.from_iterable(grad_nums))\n",
    "        grad_nums = list(pd.DataFrame(np.array(grad_nums)).melt().sort_values('value')['value'])\n",
    "            \n",
    "    else:\n",
    "        rounded = round(N*0.05)\n",
    "        num_bins = trunc(N/rounded)\n",
    "        subtract = num_bins*rounded\n",
    "        num_in_added_bin = op_mat.shape[0] - subtract\n",
    "        bin_list = []\n",
    "        for i in list(range(1, (int(num_bins)) + 1)):\n",
    "            for j in range(rounded):\n",
    "                bin_list.append(i)\n",
    "\n",
    "        grad_nums = bin_list + [int(num_bins + 1)]*int(num_in_added_bin)\n",
    "\n",
    "    g1_new_order = sub_grads[['index', 'Subtype', 'g1']].sort_values(by = 'g1')\n",
    "    g1_new_order['grad'] = grad_nums\n",
    "    g1_new_order['index_new'] = list(range(0,N))\n",
    "    \n",
    "    g2_new_order = sub_grads[['index', 'Subtype', 'g2']].sort_values(by = 'g2')\n",
    "    g2_new_order['grad'] = grad_nums\n",
    "    g2_new_order['index_new'] =  list(range(0,N))\n",
    "    \n",
    "    g3_new_order = sub_grads[['index', 'Subtype', 'g3']].sort_values(by = 'g3')\n",
    "    g3_new_order['grad'] = grad_nums\n",
    "    g3_new_order['index_new'] =  list(range(0,N))\n",
    "    \n",
    "    g4_new_order = sub_grads[['index', 'Subtype', 'g4']].sort_values(by = 'g4')\n",
    "    g4_new_order['grad'] = grad_nums\n",
    "    g4_new_order['index_new'] =  list(range(0,N))\n",
    "    \n",
    "    g5_new_order = sub_grads[['index', 'Subtype', 'g5']].sort_values(by = 'g5')\n",
    "    g5_new_order['grad'] = grad_nums\n",
    "    g5_new_order['index_new'] =  list(range(0,N))\n",
    "    \n",
    "    if op == 'main':\n",
    "         sub_class_list.main_grads_processed = g1_new_order, g2_new_order, g3_new_order, g4_new_order, g5_new_order\n",
    "    \n",
    "    elif op == 'replace':\n",
    "        sub_class_list.replace_grads_processed = g1_new_order, g2_new_order, g3_new_order, g4_new_order, g5_new_order\n",
    "        \n",
    "    elif op == 'suppress':\n",
    "        sub_class_list.suppress_grads_processed = g1_new_order, g2_new_order, g3_new_order, g4_new_order, g5_new_order\n",
    "        \n",
    "    elif op == 'clear':\n",
    "        sub_class_list.clear_grads_processed = g1_new_order, g2_new_order, g3_new_order, g4_new_order, g5_new_order\n",
    "        \n",
    "        \n",
    "    return sub_class_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from functools import partial\n",
    "\n",
    "for j in ['main', 'replace', 'suppress', 'clear']:\n",
    "    sub_class_list = *map(partial(process_grads, op=j), sub_class_list),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_process_grads = '''def process_grads(grad_fit, op_mat, num, colors, filepath):\n",
    "    \n",
    "    rc_g1=grad_fit.gradients_[num][:,0]\n",
    "    rc_g2=grad_fit.gradients_[num][:,1]\n",
    "    rc_g3=grad_fit.gradients_[num][:,2]\n",
    "    rc_g4=grad_fit.gradients_[num][:,3]\n",
    "    rc_g5=grad_fit.gradients_[num][:,4]\n",
    "    \n",
    "    Y = np.stack((rc_g1, rc_g2, rc_g3, rc_g4, rc_g5)).T\n",
    "\n",
    "    fig = plt.figure(figsize=(8.5,5.5))\n",
    "  \n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "\n",
    "    ax1.scatter(Y[:, 0], Y[:, 1], c=Y[:, 1], cmap='jet_r')\n",
    "    ax2.scatter(Y[:, 0], Y[:, 1], c=colors)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f'{filepath}_grad_axes.png')\n",
    "\n",
    "    plt.clf()\n",
    "  \n",
    "    #get_anim(Y)\n",
    "    plot_3Dfigure(Y, colors, net=True, filepath=None)\n",
    "    plot_3Dfigure(Y, colors, filepath=None)\n",
    "    \n",
    "    grads = pd.DataFrame(Y)\n",
    "    #grads.index = parcel_index\n",
    "    grads.columns = ['g1', 'g2', 'g3', 'g4', 'g5']\n",
    "    df_grads = pd.concat([op_mat, grads], axis = 1)\n",
    "    sub_grads = df_grads[['Subtype', 'g1', 'g2', 'g3', 'g4', 'g5']].reset_index()\n",
    "    #sub_grads.index = parcel_index\n",
    "\n",
    "   \n",
    "    N = int(op_mat.shape[0])\n",
    "    \n",
    "    if N == 360:\n",
    "        grad_nums = []\n",
    "        for i in range(1,21):\n",
    "            grad_nums.append(list(itertools.repeat(i, int(360/20))))\n",
    "            #grad_nums = list(itertools.chain.from_iterable(grad_nums))\n",
    "        grad_nums = list(pd.DataFrame(np.array(grad_nums)).melt().sort_values('value')['value'])\n",
    "            \n",
    "    else:\n",
    "        rounded = round(N*0.05)\n",
    "        num_bins = trunc(N/rounded)\n",
    "        subtract = num_bins*rounded\n",
    "        num_in_added_bin = op_mat.shape[0] - subtract\n",
    "        bin_list = []\n",
    "        for i in list(range(1, (int(num_bins)) + 1)):\n",
    "            for j in range(rounded):\n",
    "                bin_list.append(i)\n",
    "\n",
    "        grad_nums = bin_list + [int(num_bins + 1)]*int(num_in_added_bin)\n",
    "\n",
    "    g1_new_order = sub_grads[['index', 'Subtype', 'g1']].sort_values(by = 'g1')\n",
    "    g1_new_order['grad'] = grad_nums\n",
    "    #g1_new_order['grad'] = list(range(0,N))\n",
    "    g1_new_order['index_new'] = list(range(0,N))\n",
    "    \n",
    "    g2_new_order = sub_grads[['index', 'Subtype', 'g2']].sort_values(by = 'g2')\n",
    "    g2_new_order['grad'] = grad_nums\n",
    "    #g2_new_order['grad'] = list(range(0,N))\n",
    "    g2_new_order['index_new'] =  list(range(0,N))\n",
    "    \n",
    "    g3_new_order = sub_grads[['index', 'Subtype', 'g3']].sort_values(by = 'g3')\n",
    "    g3_new_order['grad'] = grad_nums\n",
    "    #g3_new_order['grad'] = list(range(0,N))\n",
    "    g3_new_order['index_new'] =  list(range(0,N))\n",
    "    \n",
    "    g4_new_order = sub_grads[['index', 'Subtype', 'g4']].sort_values(by = 'g4')\n",
    "    #g4_new_order['grad'] = list(range(0,N))\n",
    "    g4_new_order['grad'] = grad_nums\n",
    "    g4_new_order['index_new'] =  list(range(0,N))\n",
    "    \n",
    "    g5_new_order = sub_grads[['index', 'Subtype', 'g5']].sort_values(by = 'g5')\n",
    "    #g5_new_order['grad'] = list(range(0,N))\n",
    "    g5_new_order['grad'] = grad_nums\n",
    "    g5_new_order['index_new'] =  list(range(0,N))\n",
    "    \n",
    "    return g1_new_order, g2_new_order, g3_new_order, g4_new_order, g5_new_order\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_proccess_grads = '''pull_list = []\n",
    "for i in list(range(0,len(sub_class_list)*4, 4)):\n",
    "    pull_list.append(list(np.array(list(range(0, 4, 1))) + i))\n",
    "    \n",
    "def extract_grads(grad_fit, num):\n",
    "    \n",
    "        rc_g1=grad_fit.gradients_[num][:,0]\n",
    "        rc_g2=grad_fit.gradients_[num][:,1]\n",
    "        rc_g3=grad_fit.gradients_[num][:,2]\n",
    "        rc_g4=grad_fit.gradients_[num][:,3]\n",
    "        rc_g5=grad_fit.gradients_[num][:,4]\n",
    "    \n",
    "        Y = np.stack((rc_g1, rc_g2, rc_g3, rc_g4, rc_g5)).T\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "\n",
    "for i,j in zip(range(len(sub_class_list)), pull_list):\n",
    "    \n",
    "    sub_class_list[i].main_grads_processed = process_grads(grads, sub_class_list[i].df1, j[0], \n",
    "                                                           sub_class_list[i].colors, \n",
    "                                                           filepath= sub_class_list[i].plot_path+'_main_')\n",
    "    \n",
    "    \n",
    "    sub_class_list[i].replace_grads_processed = process_grads(grads, sub_class_list[i].df1, j[1],\n",
    "                                                              sub_class_list[i].colors, \n",
    "                                                              filepath=sub_class_list[i].plot_path+'_replace_')\n",
    "    \n",
    "    sub_class_list[i].suppress_grads_processed = process_grads(grads, sub_class_list[i].df1, j[2], \n",
    "                                                               sub_class_list[i].colors, \n",
    "                                                               sub_class_list[i].plot_path+'_suppress_')\n",
    "    \n",
    "    sub_class_list[i].clear_grads_processed = process_grads(grads, sub_class_list[i].df1, j[3], \n",
    "                                                            sub_class_list[i].colors, \n",
    "                                                            sub_class_list[i].plot_path+'_clear_')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_orders(data):\n",
    "    \n",
    "    main = data.main_grads_processed\n",
    "    replace = data.replace_grads_processed\n",
    "    suppress = data.suppress_grads_processed\n",
    "    clear = data.clear_grads_processed\n",
    "    \n",
    "    g1_new_orders=[]\n",
    "    g2_new_orders=[]\n",
    "    g3_new_orders=[]\n",
    "    for i in main, replace, suppress, clear:\n",
    "  \n",
    "        N = 360\n",
    "\n",
    "        g1 = i[0]\n",
    "        g2 = i[1]\n",
    "        g3 = i[2]\n",
    "\n",
    "        grad_color_maps_r = get_color_maps('jet_r', N, int(N/N))\n",
    "        grad_color_maps = get_color_maps('jet', N, int(N/N))\n",
    "        # add color maps to gradient dfs\n",
    "        g1_new_order = pd.merge(g1, grad_color_maps, on='index_new')\n",
    "        g2_new_order = pd.merge(g2, grad_color_maps, on='index_new')\n",
    "        g3_new_order = pd.merge(g3, grad_color_maps, on='index_new')\n",
    "        \n",
    "        op_names = i[0].name\n",
    "\n",
    "        g1_new_order['ops'] = op_names\n",
    "        g1_new_order['gradient'] = 1\n",
    "        g1_new_order.name = op_names\n",
    "\n",
    "        g2_new_order['ops'] = op_names\n",
    "        g2_new_order['gradient'] = 2\n",
    "        g2_new_order.name = op_names\n",
    "\n",
    "        g3_new_order['ops'] = op_names\n",
    "        g3_new_order['gradient'] = 3\n",
    "        g3_new_order.name = op_names\n",
    "        \n",
    "        g1_new_orders.append(g1_new_order)\n",
    "        g2_new_orders.append(g2_new_order)\n",
    "        g3_new_orders.append(g3_new_order)\n",
    "    \n",
    "    grad1_all_ops = pd.concat(g1_new_orders)\n",
    "    grad2_all_ops = pd.concat(g2_new_orders)\n",
    "    grad3_all_ops = pd.concat(g3_new_orders)\n",
    "    \n",
    "    return grad1_all_ops, grad2_all_ops, grad3_all_ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_grads(data):\n",
    "\n",
    "    grad_ops_list = []\n",
    "    grad_grad_list = []\n",
    "    grad_sub_list = []\n",
    "\n",
    "    grouped_grad = data.groupby(['ops', 'grad', 'Subtype']).count()\n",
    "\n",
    "    for i in range(0,grouped_grad.shape[0], 1):\n",
    "        grad_ops_list.append(grouped_grad.index[i][0])\n",
    "        grad_grad_list.append(grouped_grad.index[i][1])\n",
    "        grad_sub_list.append(grouped_grad.index[i][2])\n",
    "\n",
    "\n",
    "    grouped_grad = grouped_grad.reset_index(drop=True)[['index']].rename({'index':'count'},axis=1)\n",
    "    grouped_grad['ops'] = grad_ops_list\n",
    "    grouped_grad['grads'] = grad_grad_list\n",
    "    grouped_grad['subs'] = grad_sub_list\n",
    "    \n",
    "    return(grouped_grad)\n",
    "\n",
    "#grouped_grad_1 = grouped_grads(grad1_all_ops)\n",
    "#grouped_grad_2 = grouped_grads(grad2_all_ops)\n",
    "#grouped_grad_3 = grouped_grads(grad3_all_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_grads(sub_class_list[0].grad1_all_ops.query('ops == \"clear\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sub_class_list)):\n",
    "    for j in range(3):\n",
    "        sub_class_list[i].main_grads_processed[j].name = 'main'\n",
    "        sub_class_list[i].replace_grads_processed[j].name = 'replace'\n",
    "        sub_class_list[i].suppress_grads_processed[j].name = 'suppress'\n",
    "        sub_class_list[i].clear_grads_processed[j].name = 'clear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sub_class_list)):\n",
    "    sub_class_list[i].grad1_all_ops, sub_class_list[i].grad2_all_ops, sub_class_list[i].grad3_all_ops = create_new_orders(sub_class_list[i])\n",
    "    sub_class_list[i].grouped_grad_1  = grouped_grads(sub_class_list[i].grad1_all_ops)\n",
    "    sub_class_list[i].grouped_grad_2 = grouped_grads(sub_class_list[i].grad2_all_ops)\n",
    "    sub_class_list[i].grouped_grad_3  = grouped_grads(sub_class_list[i].grad3_all_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageColor\n",
    "\n",
    "cmap = cm.get_cmap('jet', 20)    # PiYG\n",
    "converted_list = []\n",
    "for i in range(cmap.N):\n",
    "    rgba = cmap(i)\n",
    "    # rgb2hex accepts rgb or rgba\n",
    "    hexc = matplotlib.colors.rgb2hex(rgba)\n",
    "    codes = ImageColor.getcolor(str(hexc), \"RGB\")\n",
    "    converted_list.append(hexc)\n",
    "\n",
    "#converted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_grad_plots(data, col, filepath=None):\n",
    "    \n",
    "    fig = plt.figure(figsize=(30, 16))\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    g = sns.catplot(\n",
    "        data=data, kind=\"violin\",\n",
    "        x=\"grad\", y=col, hue=\"grad\",\n",
    "        col=\"ops\",\n",
    "        #col_wrap = 4,\n",
    "        palette=jets, \n",
    "        #cmap = converted_list,\n",
    "        alpha=.6, \n",
    "        height=6\n",
    "    )\n",
    "    \n",
    "    (g.set_axis_labels(\"Parcel Bins\", \"Gradient Score\",  weight='bold')\n",
    "          .set_titles(\"{col_name}\", weight='bold')\n",
    "          .despine(left=True))  \n",
    "    \n",
    "    g.fig.savefig(f'{filepath}.png')\n",
    "    #plt.clf()\n",
    "    return g\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sub_class_list)):\n",
    "    bin_grad_plots(sub_class_list[i].grad1_all_ops, 'g1', sub_class_list[i].plot_path+'g1_bins')\n",
    "    bin_grad_plots(sub_class_list[i].grad2_all_ops, 'g2', sub_class_list[i].plot_path+'g2_bins')\n",
    "    bin_grad_plots(sub_class_list[i].grad3_all_ops, 'g3', sub_class_list[i].plot_path+'g3_bins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cor_mat(grad_data, ops_df, threshold=None, cosine=None,  filepath=None):\n",
    "    \n",
    "    op = ops_df.name\n",
    "\n",
    "    test = grad_data.query('ops == '+'\"'+op+'\"'+'')[['index', 'grad']]\n",
    "    test_merge = pd.merge(test, ops_df.reset_index(), on = 'index').sort_values('grad').drop('index', axis=1)#.reset_index(drop=True)\n",
    "\n",
    "    X = np.array(test_merge)\n",
    "    X_Z = (X - np.mean(X, axis=0, keepdims=True)) / np.std(X, axis=0, dtype=np.float64, ddof=1, keepdims=True)\n",
    "    D, rho = spearmanr(np.array(X_Z), axis=1)\n",
    "    \n",
    "    if threshold is not None:\n",
    "        perc = np.array([np.percentile(x, 90) for x in D])\n",
    "\n",
    "        for i in range(D.shape[0]):\n",
    "            D[i, D[i,:] < perc[i]] = 0  \n",
    "            \n",
    "    \n",
    "    if cosine is not None:\n",
    "\n",
    "        D = 1 - pairwise_distances(D, metric = 'cosine')\n",
    "\n",
    "    g = sns.clustermap(D, \n",
    "                           row_colors= get_colors('jet', 360),\n",
    "                           col_colors=get_colors('jet', 360),\n",
    "                           metric = 'cosine',\n",
    "                           row_cluster=False, col_cluster=False, \n",
    "                           cmap = 'seismic',\n",
    "                           center=0,\n",
    "                           vmin=-1, vmax=1,\n",
    "                           linewidths=0, xticklabels=False, yticklabels=False,\n",
    "                           #**kws\n",
    "                          )\n",
    "    if filepath is not None:\n",
    "        g.fig.savefig(f'{filepath}.png')\n",
    "        \n",
    "    plt.clf()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sub_class_list)):\n",
    "    for j,n in zip([sub_class_list[i].main, sub_class_list[i].replace,  sub_class_list[i].suppress,  sub_class_list[i].clear],\n",
    "                   ['main', 'replace', 'suppress', 'clear']):\n",
    "        \n",
    "        grad_cor_mat(sub_class_list[i].grad1_all_ops, j, \n",
    "                     filepath=sub_class_list[i].plot_path+n+'_cormat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_means(data, ops_df, op):\n",
    "\n",
    "    test = data.query('ops == '+'\"'+op+'\"'+'')[['index', 'grad']]\n",
    "    test_merge = pd.merge(test, ops_df.reset_index(), on = 'index').drop('index', axis=1)\n",
    "\n",
    "    #cor_mat = tester_merge.groupby('grad').mean().T.corr().reset_index(drop=True)\n",
    "    #sns.clustermap(cor_mat,row_colors=jets,col_colors=jets,cmap = 'seismic')\n",
    "\n",
    "    merged_mean = test_merge.groupby('grad').mean().reset_index(drop=True).T\n",
    "\n",
    "    final_mean = pd.DataFrame(merged_mean.mean()).rename({0:'mean'}, axis=1)\n",
    "    final_mean['grad'] = list(range(1,21,1))\n",
    "    final_mean['ops'] = op\n",
    "    \n",
    "    return final_mean\n",
    "\n",
    "def get_gradient_means(data):\n",
    "\n",
    "#sns.lineplot(data=xmean, x=\"grad\", y=\"mean\")\n",
    "    g1_rsa_ops_means = []\n",
    "    g2_rsa_ops_means = []\n",
    "    g3_rsa_ops_means = []\n",
    "\n",
    "    for i in [\"main\", \"replace\", \"suppress\", \"clear\"]:\n",
    "\n",
    "        if i == \"main\":\n",
    "            ops_df = data.main\n",
    "\n",
    "        elif i == \"replace\":\n",
    "            ops_df = data.replace\n",
    "\n",
    "        elif i == \"suppress\":\n",
    "            ops_df = data.suppress\n",
    "\n",
    "        elif i == \"clear\":\n",
    "            ops_df = data.clear\n",
    "\n",
    "        output1 = grad_means(data.grad1_all_ops, ops_df, i)\n",
    "        output2 = grad_means(data.grad2_all_ops, ops_df, i)\n",
    "        output3 = grad_means(data.grad3_all_ops, ops_df, i)\n",
    "\n",
    "        g1_rsa_ops_means.append(output1)\n",
    "        g2_rsa_ops_means.append(output2)\n",
    "        g3_rsa_ops_means.append(output3)\n",
    "\n",
    "    g1_rsa_ops_means_2 = pd.concat(g1_rsa_ops_means).reset_index(drop=True)\n",
    "    g2_rsa_ops_means_2 = pd.concat(g2_rsa_ops_means).reset_index(drop=True)\n",
    "    g3_rsa_ops_means_2 = pd.concat(g3_rsa_ops_means).reset_index(drop=True)\n",
    "    \n",
    "    data.g1_rsa_ops_means_2 = g1_rsa_ops_means_2\n",
    "    data.g2_rsa_ops_means_2 = g2_rsa_ops_means_2\n",
    "    data.g3_rsa_ops_means_2 = g3_rsa_ops_means_2\n",
    "    \n",
    "    return  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_class_list = *map(get_gradient_means, sub_class_list),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sub_class_list)):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 3))\n",
    "    sns.set_theme(style=\"darkgrid\")\n",
    "    g = sns.lineplot(data=sub_class_list[i].g1_rsa_ops_means_2, x=\"grad\", y=\"mean\", hue = \"ops\", palette = ops_cols)\n",
    "    #fig.savefig(sub_class_list[i].plot_path+j+'_g1_rsa_means.png')\n",
    "    plt.clf()\n",
    "    fig2 = plt.figure(figsize=(10, 3))\n",
    "    g2 = sns.lineplot(data=sub_class_list[i].g2_rsa_ops_means_2, x=\"grad\", y=\"mean\", hue = \"ops\", palette = ops_cols)\n",
    "    #fig2.savefig(sub_class_list[i].plot_path+j+'_g2_rsa_means.png')\n",
    "    plt.clf()\n",
    "    fig3 = plt.figure(figsize=(10, 3))\n",
    "    g3 = sns.lineplot(data=sub_class_list[i].g3_rsa_ops_means_2, x=\"grad\", y=\"mean\", hue = \"ops\", palette = ops_cols)\n",
    "    #fig3.savefig(sub_class_list[i].plot_path+j+'_g3_rsa_means.png')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_grad_plots(data, col, filepath):\n",
    "    \n",
    "    fig = plt.figure(figsize=(30, 16))\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    g = sns.catplot(\n",
    "        data=data, kind=\"violin\",\n",
    "        x=\"Subtype\", y=col, hue=\"Subtype\",\n",
    "        col=\"ops\",\n",
    "        #col_wrap = 4,\n",
    "        palette=net_cols, \n",
    "        alpha=.2, \n",
    "        height=6\n",
    "    )\n",
    "    \n",
    "    (g.set_axis_labels(\"Network\", \"Gradient Score\",  weight='bold')\n",
    "      .set_titles(\"{col_name}\", weight='bold')\n",
    "      .despine(left=True))  \n",
    "\n",
    "    if filepath is not None:\n",
    "        g.fig.savefig(f'{filepath}.png')\n",
    "    \n",
    "    return g\n",
    "    \n",
    "#sub_grad_plots(grad1_all_ops, 'g1')\n",
    "#sub_grad_plots(grad2_all_ops, 'g2')\n",
    "#sub_grad_plots(grad3_all_ops, 'g3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sub_class_list)):\n",
    "    fig = plt.figure(figsize=(10, 3))\n",
    "    sns.set_theme(style=\"darkgrid\")\n",
    "    g = sub_grad_plots(sub_class_list[i].grad1_all_ops, 'g1', sub_class_list[i].plot_path+'nets_g1')\n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(10, 3))\n",
    "    sns.set_theme(style=\"darkgrid\")\n",
    "    g = sub_grad_plots(sub_class_list[i].grad2_all_ops, 'g2', sub_class_list[i].plot_path+'nets_g2')\n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(10, 3))\n",
    "    sns.set_theme(style=\"darkgrid\")\n",
    "    g = sub_grad_plots(sub_class_list[i].grad3_all_ops, 'g3', sub_class_list[i].plot_path+'nets_g3')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb_grads(data, op):\n",
    "    \n",
    "    x1 = data.grad1_all_ops.query('ops == '+'\"'+op+'\"'+'')[['index', 'Subtype', 'g1']]\n",
    "    x2 = data.grad2_all_ops.query('ops == '+'\"'+op+'\"'+'')[['index', 'g2']]\n",
    "    x3 = data.grad3_all_ops.query('ops == '+'\"'+op+'\"'+'')[['index', 'g3']]\n",
    "\n",
    "    merged_grads = pd.merge(x1, x2, on ='index')\n",
    "    merged_grads = pd.merge(merged_grads, x3, on ='index').sort_values('index').reset_index(drop=True)\n",
    "    \n",
    "    if op == \"main\":\n",
    "        data.main_grads = merged_grads \n",
    "    elif op == \"replace\":\n",
    "        data.replace_grads = merged_grads \n",
    "    elif op == \"suppress\":\n",
    "        data.suppress_grads = merged_grads \n",
    "    elif op == \"clear\":\n",
    "        data.clear_grads = merged_grads \n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_all_grad_ops(data, g):\n",
    "    \n",
    "    grad_all = pd.merge(data.main_grads[['index', g]], data.replace_grads[['index', g]], on = 'index')\n",
    "    grad_all = pd.merge(grad_all, data.suppress_grads[['index', g]], on = 'index')\n",
    "    grad_all = pd.merge(grad_all, data.clear_grads[['index', g]], on = 'index')\n",
    "    grad_all.columns = ['index', 'maintain', 'replace', 'suppress', 'clear']\n",
    "    grad_all_melt = grad_all.melt(id_vars=['index']).rename({'variable':'ops', 'value':'gradient'}, axis =1)\n",
    "    \n",
    "    if g == 'g1':\n",
    "        data.g1_all = grad_all\n",
    "        data.g1_all_melt = grad_all_melt\n",
    "    elif g == 'g2':\n",
    "        data.g2_all = grad_all\n",
    "        data.g2_all_melt = grad_all_melt\n",
    "    elif g == 'g3':\n",
    "        data.g3_all = grad_all\n",
    "        data.g3_all_melt = grad_all_melt\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in ['main', 'replace', 'suppress', 'clear']:\n",
    "    sub_class_list = *map(partial(comb_grads, op=j), sub_class_list),\n",
    "    \n",
    "for j in ['g1', 'g2', 'g3']:\n",
    "    sub_class_list = *map(partial(get_all_grad_ops, g=j), sub_class_list),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_loop='''\n",
    "for i in range(len(sub_class_list)):\n",
    "    for j in [\"main\", \"replace\", \"suppress\", \"clear\"]:\n",
    "        sub_class_list[i] =  comb_grads(sub_class_list[i], j)\n",
    "\n",
    "for i in range(len(sub_class_list)):\n",
    "    for j in ['g1', 'g2', 'g3']:\n",
    "        sub_class_list[i] =  get_all_grad_ops(sub_class_list[i], j)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sub_class_list)):\n",
    "    for j,k in zip([sub_class_list[i].g1_all_melt, \n",
    "                    sub_class_list[i].g2_all_melt, \n",
    "                    sub_class_list[i].g3_all_melt], \n",
    "                   ['g1', 'g2', 'g3']):\n",
    "        \n",
    "        sns.set_style(\"darkgrid\")\n",
    "        g = sns.displot(data=j, x='gradient',hue='ops', kind='kde',\n",
    "                   linewidth=3, palette=ops_cols)\n",
    "        \n",
    "        (g.set_axis_labels(\"Gradient Score\", \"Density\",  weight='bold')\n",
    "          .set_titles(\"{col_name}\", weight='bold')\n",
    "          .despine(left=True))  \n",
    "            \n",
    "        g.tight_layout()\n",
    "        g.savefig(sub_class_list[i].plot_path+k+'_hist.png')\n",
    "        \n",
    "        plt.clf()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sub_class_list)):\n",
    "    sub_class_list[i].main_grads.name = 'main'  \n",
    "    sub_class_list[i].replace_grads.name = 'replace'\n",
    "    sub_class_list[i].suppress_grads.name = 'suppress'\n",
    "    sub_class_list[i].clear_grads.name = 'clear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist(data, op, node_list, squared):\n",
    "    \n",
    "    if op == 'main':\n",
    "        new_data = data.main_grads[['g1', 'g2', 'g3']]\n",
    "        new_data.name = data.main_grads.name\n",
    "        \n",
    "    if op == 'replace':\n",
    "        new_data = data.replace_grads[['g1', 'g2', 'g3']]\n",
    "        new_data.name = data.replace_grads.name\n",
    "        \n",
    "    if op == 'suppress':\n",
    "        new_data = data.suppress_grads[['g1', 'g2', 'g3']]\n",
    "        new_data.name = data.suppress_grads.name\n",
    "            \n",
    "    if op == 'clear':\n",
    "        new_data = data.clear_grads[['g1', 'g2', 'g3']]\n",
    "        new_data.name = data.clear_grads.name\n",
    "    \n",
    "    from sklearn.metrics.pairwise import euclidean_distances\n",
    "    euc = euclidean_distances(new_data, squared=squared)\n",
    "    euc_copy = euc.copy()\n",
    "    tril = np.triu_indices(len(euc))\n",
    "    euc[tril] = np.nan\n",
    "    eucm = pd.DataFrame(euc).melt().dropna().reset_index(drop=True)\n",
    "\n",
    "    names=[]\n",
    "    nodes1=[]\n",
    "    nodes2=[]\n",
    "    for name in itertools.combinations(node_list,2):\n",
    "        node1 = name[0]\n",
    "        node2 = name[1]\n",
    "        names.append(name)\n",
    "        nodes1.append(node1)\n",
    "        nodes2.append(node2)\n",
    "\n",
    "    eucm['pair'] = names\n",
    "    eucm['node1'] = nodes1\n",
    "    eucm['node2'] = nodes2\n",
    "    \n",
    "    if new_data.name == 'main':\n",
    "        data.main_dist_mask = euc_copy\n",
    "        data.main_dist =  eucm\n",
    "        \n",
    "    if new_data.name == 'replace':\n",
    "        data.replace_dist_mask = euc_copy\n",
    "        data.replace_dist =  eucm\n",
    "        \n",
    "    if new_data.name == 'suppress':\n",
    "            data.suppress_dist_mask = euc_copy\n",
    "            data.suppress_dist =  eucm\n",
    "            \n",
    "    if new_data.name == 'clear':\n",
    "            data.clear_dist_mask = euc_copy\n",
    "            data.clear_dist =  eucm\n",
    "            \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in [\"main\", \"replace\", \"suppress\", \"clear\"]:\n",
    "     sub_class_list = *map(partial(get_dist, op=z, node_list = range(360), squared=False), sub_class_list),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_additions(data):\n",
    "    data.clear_suppress_dist = data.clear_dist['value'] - data.suppress_dist['value']\n",
    "    data.clear_replace_dist = data.clear_dist['value'] - data.replace_dist['value']\n",
    "    data.clear_main_dist = data.clear_dist['value'] - data.main_dist['value']\n",
    "\n",
    "    data.suppress_replace_dist = data.suppress_dist['value'] - data.replace_dist['value']\n",
    "    data.suppress_main_dist = data.suppress_dist['value'] - data.main_dist['value']\n",
    "\n",
    "    data.replace_main_dist = data.replace_dist['value'] - data.main_dist['value']\n",
    "    \n",
    "    data.clear_dist_df = pd.DataFrame(data.clear_dist['value']).rename({'value':'clear_dist'}, axis=1)\n",
    "    data.suppress_dist_df = pd.DataFrame(data.suppress_dist['value']).rename({'value':'suppress_dist'}, axis=1)\n",
    "    data.replace_dist_df = pd.DataFrame(data.replace_dist['value']).rename({'value':'replace_dist'}, axis=1)\n",
    "    data.main_dist_df = pd.DataFrame(data.main_dist['value']).rename({'value':'main_dist'}, axis=1)\n",
    "\n",
    "    data.op_dist_all = pd.concat([data.clear_dist_df, data.suppress_dist_df, \n",
    "                                  data.replace_dist_df, data.main_dist_df], axis =1).melt()\n",
    "    data.op_dist_all.columns = ['ops', 'dist']\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_class_list = *map(distance_additions, sub_class_list),\n",
    "\n",
    "for i in range(len(sub_class_list)):\n",
    "    sub_class_list[i].dist_aov = getF(sub_class_list[i].op_dist_all, 'dist', 'ops').round(3)\n",
    "    sub_class_list[i].dist_post_hoc = getF(sub_class_list[i].op_dist_all, 'dist', 'ops').round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(sub_class_list)):\n",
    "#        sub_class_list[i] = distance_additions(sub_class_list[i])\n",
    "#        sub_class_list[i].dist_aov = getF(sub_class_list[i].op_dist_all, 'dist', 'ops').round(3)\n",
    "#        sub_class_list[i].dist_post_hoc = getF(sub_class_list[i].op_dist_all, 'dist', 'ops').round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sub_class_list)):\n",
    "\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    g = sns.displot(data=sub_class_list[i].op_dist_all, x='dist',hue='ops', kind='kde', palette=ops_cols, linewidth=3)\n",
    "    \n",
    "    (g.set_axis_labels(\"Parcel Pairwise Centrality\", \"Density\",  weight='bold')\n",
    "          .set_titles(\"{col_name}\", weight='bold')\n",
    "          .despine(left=True))  \n",
    "    g.tight_layout()\n",
    "    g.savefig(sub_class_list[i].plot_path+'paircent_all_ops_hist.png')\n",
    "        \n",
    "    plt.clf()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_class_list = list(sub_class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_class_list_copy = sub_class_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_centrality(data, node_list):\n",
    "    cent_outs = []\n",
    "    for i in node_list:\n",
    "        t1 = data[data['node1'] == i]\n",
    "        t2 = data[data['node2'] == i]\n",
    "        frame = pd.concat([t1, t2])['value'].mean()\n",
    "        cent_outs.append(frame)\n",
    "\n",
    "    cent_frame = pd.DataFrame(cent_outs).rename({0:'centrality'}, axis=1)\n",
    "    \n",
    "    return cent_frame\n",
    "\n",
    "def add_op_centraility(data):\n",
    "    data.main_centrality = node_centrality(data.main_dist, range(360))\n",
    "    data.replace_centrality = node_centrality(data.replace_dist, range(360))\n",
    "    data.suppress_centrality = node_centrality(data.suppress_dist, range(360))\n",
    "    data.clear_centrality = node_centrality(data.clear_dist, range(360))\n",
    "\n",
    "    data.all_centraility = pd.concat([data.main_centrality, data.replace_centrality, \n",
    "                                 data.suppress_centrality, data.clear_centrality], axis=1)\n",
    "\n",
    "    data.all_centraility.columns = ['main', 'replace', 'suppress', 'clear']\n",
    "    \n",
    "    return data\n",
    "\n",
    "def move_op_centrality(data):\n",
    "    \n",
    "    data.main_grads['centrality'] = data.main_centrality #* -1\n",
    "    data.replace_grads['centrality'] = data.replace_centrality #* -1\n",
    "    data.suppress_grads['centrality'] = data.suppress_centrality #* -1\n",
    "    data.clear_grads['centrality'] = data.clear_centrality #* -1\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "for i in range(len(sub_class_list)):\n",
    "    sub_class_list[i] = add_op_centraility(sub_class_list[i])\n",
    "    \n",
    "    g = sns.displot(data=sub_class_list[i].all_centraility.melt(), \n",
    "                    x='value',hue='variable', kind='kde', palette=ops_cols, linewidth=3)\n",
    "    \n",
    "    (g.set_axis_labels(\"Parcel Average Centrality\", \"Density\",  weight='bold')\n",
    "          .set_titles(\"{col_name}\", weight='bold')\n",
    "          .despine(left=True))  \n",
    "    g.tight_layout()\n",
    "    g.savefig(sub_class_list[i].plot_path+'avgcent_all_ops_hist.png')\n",
    "\n",
    "    \n",
    "    #g.savefig(sub_class_list[0].plot_path +'op_centrality_hist.png')\n",
    "    plt.clf()\n",
    "    sub_class_list[i] = move_op_centrality(sub_class_list[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_cent_difs(data):\n",
    "\n",
    "    network_cent_aovs=[]\n",
    "    network_cent_difs=[]\n",
    "\n",
    "    for i in data.main_grads, data.replace_grads, data.suppress_grads, data.clear_grads:\n",
    "\n",
    "        aov_c = pd.DataFrame(getF(i, 'centrality', 'Subtype')).T.round(3)\n",
    "        aov_c.columns = ['F', 'p']\n",
    "\n",
    "        post_hocs_c = pd.DataFrame(getposthoc(i, 'centrality', 'Subtype')).round(3)\n",
    "        post_hocs_c.columns = post_hocs_c.index\n",
    "        #post_hocs_c['op'] = ['main', 'suppress', 'suppress', 'clear']\n",
    "\n",
    "        network_cent_aovs.append(aov_c)\n",
    "        network_cent_difs.append(post_hocs_c)\n",
    "\n",
    "    data.network_cent_aov_out = pd.concat(network_cent_aovs)\n",
    "    data.network_cent_aov_out['op'] = ['main', 'suppress', 'suppress','clear']\n",
    "\n",
    "\n",
    "    data.network_cent_difs_out = pd.concat(network_cent_difs)\n",
    "    data.network_cent_difs_out['op'] = list(itertools.chain.from_iterable(\n",
    "        [['main']*4,['replace']*4, ['suppress']*4, ['clear']*4]))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sub_class_list)):\n",
    "    sub_class_list[i] = network_cent_difs(sub_class_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_class_list = list(sub_class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D_centraility(data, filename=None):\n",
    "    \n",
    "    op_name = data.name\n",
    "        \n",
    "    newX = np.array(data[['g1', 'g2', 'g3', 'centrality']])\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    data = go.Scatter3d(x=newX[:,0], y=newX[:,1], z=newX[:,2], \n",
    "                        mode='markers',\n",
    "                        marker=dict(size=5,\n",
    "                                    color=newX[:,3],\n",
    "                                    opacity=0.7,\n",
    "                                    colorscale='inferno')\n",
    "                       )\n",
    "\n",
    "    layout = go.Layout(title_text=op_name,title_x=0.5,title_y=0.8,title_font_size=20)\n",
    "    fig = go.Figure(data=[data], layout=layout)\n",
    "    fig.update_layout(showlegend=False)\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(showticklabels=False)\n",
    "    fig.update_layout(scene = dict(\n",
    "                xaxis = dict(title= '', ticks= '', showticklabels= False,),\n",
    "                yaxis = dict(title= '', ticks= '', showticklabels= False,),\n",
    "                zaxis = dict(title= '', ticks= '', showticklabels= False,),\n",
    "                ))\n",
    "    fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "    #fig.show()\n",
    "\n",
    "    if filename is not None:\n",
    "        fig.write_html(filename+op_name+'_centrality.html')\n",
    "        \n",
    "    \n",
    "for j in range(len(sub_class_list)): \n",
    "    for i in sub_class_list[j].main_grads, sub_class_list[j].replace_grads, sub_class_list[j].suppress_grads, sub_class_list[j].clear_grads:\n",
    "\n",
    "        plot_3D_centraility(i, sub_class_list[j].plot_path)\n",
    "        #sns.set_style(\"darkgrid\")    \n",
    "        g = sns.displot(data=i, x= 'centrality', hue='Subtype', kind='kde', palette=net_cols, linewidth=3)\n",
    "\n",
    "        g.fig.suptitle(i.name, weight='bold')\n",
    "        (g.set_axis_labels(\"Network Centrality\", \"Density\",  weight='bold')\n",
    "          #.set_titles({\"i.name\"}, weight='bold')\n",
    "          .despine(left=True))  \n",
    "        g.tight_layout()\n",
    "        g.savefig(sub_class_list[j].plot_path+i.name+'_network_cent_all_ops_hist.png')\n",
    "        plt.clf()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_centraility = '''\n",
    "- For each parcel, centrality was calculated as the average Euclidean distance to all other parcels \n",
    "in the 3D gradient space (accounting for the full 3D space and not one gradient only).\n",
    "\n",
    "- In this context, high centrality refers to the smallest distance to all other parcels in space \n",
    "(i.e. towards the center of the 3D gradient space), and thus indicates a functional connectivity \n",
    "profile that isn't differentiated across all three gradients.\n",
    "'''\n",
    "\n",
    "within_network_dispersion = '''\n",
    "- Within-network dispersion was calculated as the sum of the squared Euclidean distances in the 3D \n",
    "gradient space of all parcels within that network to the network centroid (i.e., the mean coordinates\n",
    "in 3D gradient space of all parcels belonging to that network). \n",
    "\n",
    "- A small dispersion value could be interpreted as a highly integrated network, segregated from \n",
    "other networks. These multi-dimensional gradient metrics are motivated by prior related work on \n",
    "network integration and segregation, are assumed to reflect segregation of functional networks, \n",
    "and have been demonstrated to be comparable with other approaches of measuring network changes \n",
    "such as clustering, as well as within-network connectivity and segregation (Bethlehem et al., 2020).\n",
    "\n",
    "- Particularly, Euclidean distance in the 3D gradient space reflects the similarity of connectivity\n",
    "profiles between cortical parcellations, across multiple axes of differentiation. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_op_centrality(data, op):\n",
    "        \n",
    "    if op == 'main':\n",
    "        new_data = data.main_grads\n",
    "    elif op == 'replace':\n",
    "        new_data = data.replace_grads\n",
    "    elif op == 'suppress':\n",
    "        new_data = data.suppress_grads     \n",
    "    elif op == 'clear':\n",
    "        new_data = data.clear_grads\n",
    "    \n",
    "    sub_dis=[]\n",
    "    sub_dis_mask = []\n",
    "    sub_centrality = []\n",
    "    \n",
    "    for i in [1,2,3,4]:\n",
    "        test = new_data.query('Subtype =='+str(i))\n",
    "        index_lists = test['index'].to_list()\n",
    "        test_grads = test[['g1', 'g2', 'g3']]\n",
    "        test_mean = test_grads.mean()\n",
    "\n",
    "        dis_out=[]\n",
    "        for j in index_lists:\n",
    "            filtered = test.query('index =='+str(j))[['g1', 'g2', 'g3']]\n",
    "            l = pd.concat([filtered,pd.DataFrame(test_mean).T])\n",
    "            #print(l)\n",
    "            euc = np.array(euclidean_distances(l, squared=True)[0][1])\n",
    "            dis_out.append(euc)\n",
    "            \n",
    "        dispersion = pd.DataFrame(dis_out)\n",
    "        dispersion['index'] = index_lists\n",
    "        #dispersion_mask, dispersion = get_dist(test, index_lists, True)\n",
    "        #sub_cents = node_centrality(dispersion, index_lists)\n",
    "        dispersion['Subtype'] = i\n",
    "        dispersion['op'] = op\n",
    "        \n",
    "        #sub_centrality.append(sub_cents)\n",
    "        sub_dis.append(dispersion)\n",
    "        #sub_dis_mask.append(dispersion_mask)\n",
    "    \n",
    "    sub_dispersion = pd.concat(sub_dis).reset_index(drop=True).rename({0:'dispersion'}, axis = 1)\n",
    "    \n",
    "    if op == 'main':\n",
    "        data.main_op_disp = sub_dispersion\n",
    "    elif op == 'replace':\n",
    "        data.replace_op_disp = sub_dispersion\n",
    "    elif op == 'suppress':\n",
    "        data.suppress_op_disp = sub_dispersion     \n",
    "    elif op == 'clear':\n",
    "        data.clear_op_disp = sub_dispersion\n",
    "        \n",
    "    return data\n",
    "        \n",
    "    \n",
    "#main_op_disp = sub_op_centrality(main_grads, 'maintain')  \n",
    "#replace_op_disp = sub_op_centrality(replace_grads, 'replace')  \n",
    "#suppress_op_disp =  sub_op_centrality(suppress_grads, 'suppress')  \n",
    "#clear_op_disp = sub_op_centrality(clear_grads, 'clear')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sub_class_list)):\n",
    "    for z in ['main', 'replace', 'suppress', 'clear']:\n",
    "        sub_class_list[i] = sub_op_centrality(sub_class_list[i], z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_disp_difs(data):\n",
    "\n",
    "    network_disp_aovs=[]\n",
    "    network_disp_difs=[]\n",
    "\n",
    "    for i in range(1,5,1):\n",
    "        sub_disp_frame = pd.concat([\n",
    "           data.main_op_disp.query('Subtype =='+str(i)),\n",
    "           data.replace_op_disp.query('Subtype == '+str(i)),\n",
    "           data.suppress_op_disp.query('Subtype == '+str(i)),\n",
    "           data.clear_op_disp.query('Subtype == '+str(i))\n",
    "        ])\n",
    "\n",
    "        aov = pd.DataFrame(getF(sub_disp_frame, 'dispersion', 'op')).T.round(3)\n",
    "        aov.columns = ['F', 'p']\n",
    "        aov['Subtype'] = i\n",
    "\n",
    "        post_hocs = pd.DataFrame(getposthoc(sub_disp_frame, 'dispersion', 'op')).round(3)\n",
    "        post_hocs.columns = post_hocs.index\n",
    "        post_hocs['Subtype'] = i\n",
    "\n",
    "        network_disp_aovs.append(aov)\n",
    "        network_disp_difs.append(post_hocs)\n",
    "\n",
    "    data.network_disp_aov_out = pd.concat(network_disp_aovs)\n",
    "    data.network_disp_difs_out = pd.concat(network_disp_difs)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sub_class_list)):\n",
    "    sub_class_list[i] = network_disp_difs(sub_class_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sub_class_list)):\n",
    "    for j,k in zip([sub_class_list[i].main_op_disp, sub_class_list[i].replace_op_disp, sub_class_list[i].suppress_op_disp, sub_class_list[i].clear_op_disp],\n",
    "                   ['main', 'replace', 'suppress', 'clear']):\n",
    "            \n",
    "        sns.set_style(\"darkgrid\")\n",
    "        g =  sns.displot(j, x= 'dispersion', hue='Subtype', kind='kde', palette=net_cols, linewidth=3)\n",
    "        \n",
    "        g.fig.suptitle(k, weight='bold')\n",
    "        (g.set_axis_labels(\"Network Dispersion\", \"Density\",  weight='bold')\n",
    "          #.set_titles({\"i.name\"}, weight='bold')\n",
    "          .despine(left=True))  \n",
    "        g.tight_layout()\n",
    "        g.savefig(sub_class_list[i].plot_path+'_'+k+'_network_disp_all_ops_hist.png')\n",
    "        \n",
    "        plt.clf()\n",
    "           # g.savefig('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/leiden/analysis/gradients/figures/dispersion/'+j+'.png')\n",
    "            \n",
    "    #set_ylim(0, 120)\n",
    "    #sns.displot(data=i, x= 'dispersion', hue='grad1', kind='kde', palette=jets, linewidth=3)\n",
    "    #sns.displot(data=i, x= 'dispersion', hue='grad2', kind='kde', palette=jets, linewidth=3)\n",
    "    #sns.displot(data=i, x= 'dispersion', hue='grad3', kind='kde', palette=jets, linewidth=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_frames(data):\n",
    "    \n",
    "    data.clear_replace_dist_df = pd.DataFrame(data.clear_replace_dist).rename({'value':'clear_replace'}, axis=1)\n",
    "    data.clear_suppress_dist_df = pd.DataFrame(data.clear_suppress_dist).rename({'value':'clear_suppress'}, axis=1)\n",
    "    data.clear_main_dist_df = pd.DataFrame(data.clear_main_dist).rename({'value':'clear_main'}, axis=1)\n",
    "\n",
    "    data.suppress_main_dist_df = pd.DataFrame(data.suppress_main_dist).rename({'value':'suppress_main'}, axis=1)\n",
    "    data.suppress_replace_dist_df = pd.DataFrame(data.suppress_replace_dist).rename({'value':'suppress_replace'}, axis=1)\n",
    "\n",
    "    data.replace_main_dist_df = pd.DataFrame(data.replace_main_dist).rename({'value':'replace_main'}, axis=1)\n",
    "\n",
    "    data.op_dists_all = pd.concat([data.clear_replace_dist_df, data.clear_suppress_dist_df, \n",
    "                                   data.clear_main_dist_df, data.suppress_main_dist_df, \n",
    "                                   data.suppress_replace_dist_df,data.replace_main_dist_df], axis=1).melt()\n",
    "\n",
    "    data.op_dists_all.columns = ['op_comp', 'dist']\n",
    "\n",
    "    #sns.histplot(op_dists_all, x = 'dist', hue = 'op_comp')\n",
    "\n",
    "    #sns.displot(op_dists_all, x = 'dist', hue = 'op_comp', linewidth=2, kind='kde')\n",
    "\n",
    "    data.dist_op_comp_aov = getF(data.op_dists_all, 'dist', 'op_comp').round(3)\n",
    "\n",
    "    data.dist_op_comp_posthoc = getposthoc(data.op_dists_all, 'dist', 'op_comp').round(3)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sub_class_list)):\n",
    "    sub_class_list[i] = add_distance_frames(sub_class_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sub_class_list)):\n",
    "    g = sns.FacetGrid(sub_class_list[i].op_dists_all, col=\"op_comp\", hue=\"op_comp\", col_wrap=3)\n",
    "    g.map(sns.distplot, \"dist\")\n",
    "    g.set_titles(col_template=\"{col_name}\", row_template=\"\") \n",
    "    \n",
    "    g.fig.suptitle('Operation Distance Comparisons', weight='bold')\n",
    "    (g.set_axis_labels(\"Distance Change\", \"Density\",  weight='bold')\n",
    "     #.set_titles({\"i.name\"}, weight='bold')\n",
    "     .despine(left=True))  \n",
    "    g.tight_layout()\n",
    "    g.savefig(sub_class_list[i].plot_path+'op_dist_comps_hist.png')\n",
    "    \n",
    "    plt.clf()\n",
    "    #g.savefig(\"/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/leiden/analysis/gradients/figures/op_dist_comps/op_comp_all_wrapped.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sub_class_list)):\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    g = sns.displot(sub_class_list[i].op_dists_all, x= 'dist', hue='op_comp', kind = 'kde', linewidth=3)\n",
    "    \n",
    "    g.fig.suptitle('Operation Distance Comparisons', weight='bold')\n",
    "    (g.set_axis_labels(\"Distance Change\", \"Density\",  weight='bold')\n",
    "     #.set_titles({\"i.name\"}, weight='bold')\n",
    "     .despine(left=True))  \n",
    "    g.tight_layout()\n",
    "    g.savefig(sub_class_list[i].plot_path+'all_op_dist_comps_hist.png')\n",
    "    \n",
    "    plt.clf()\n",
    "    #g.savefig(\"/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/leiden/analysis/gradients/figures/op_dist_comps/op_comp_all_kde.png\")\n",
    "    #g = sns.displot(sub_class_list[i].op_dists_all, x= 'dist', hue='op_comp', kind = 'ecdf', linewidth=3)   \n",
    "    #plt.clf()\n",
    "    #g.savefig(\"/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/leiden/analysis/gradients/figures/op_dist_comps/op_comp_all_ecdf.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_change(grad_data, dist_mat, parcel_index, colors,title, filepath=None):\n",
    "    sns.set_style('white')\n",
    "    sorted_subtype_parcels = grad_data\n",
    "    color_df = pd.DataFrame(colors).reset_index().rename({'Subtype':'Networks'}, axis = 1)\n",
    "    color_df.index = parcel_index\n",
    "    color_df['index'] = parcel_index\n",
    "    sorted_subtype_parcels = pd.merge(sorted_subtype_parcels , color_df, on ='index').sort_values('Subtype')#.reset_index(drop=True)\n",
    "    sorted_subtype_parcels_list = sorted_subtype_parcels['index'].to_list()\n",
    "    network_colors = sorted_subtype_parcels['Networks'].to_list()\n",
    "    \n",
    "    change_dist_mat = vsim(dist_mat)\n",
    "    \n",
    "    change_dist_mat.columns = parcel_index\n",
    "    change_dist_mat.index = parcel_index\n",
    "    \n",
    "    change_dist_mat = change_dist_mat[sorted_subtype_parcels_list].T\n",
    "    change_dist_mat = change_dist_mat[sorted_subtype_parcels_list].T\n",
    "    \n",
    "    #print(change_dist_mat)\n",
    "    #change_dist_mat = threshold_proportional(np.array(change_dist_mat), .75)\n",
    "    \n",
    "    change_dist_mat = np.array(change_dist_mat)\n",
    "    \n",
    "    #perc = np.array([np.percentile(x, 90) for x in change_dist_mat])\n",
    "\n",
    "    #for i in range(change_dist_mat.shape[0]):\n",
    "    #    change_dist_mat[i, change_dist_mat[i,:] < perc[i]] = 0    \n",
    "        \n",
    "\n",
    "    matrix = pd.DataFrame(np.tril(change_dist_mat))\n",
    "    #matrix = change_dist_mat\n",
    "    \n",
    "    #change_dist_mat = threshold_proportional(change_dist_mat, .15)\n",
    "    \n",
    "    #kws = dict(cbar_kws=dict(ticks=[-.03, 0, .03]), figsize=(6, 6))\n",
    "    kws = dict(cbar_kws=dict(ticks=[-.3, 0, .1]), figsize=(6, 6))\n",
    "    g = sns.clustermap(matrix, \n",
    "                       row_colors=network_colors,\n",
    "                       #metric = 'cosine',\n",
    "                       col_colors=network_colors,\n",
    "                       row_cluster=False, col_cluster=False, \n",
    "                       cmap = 'seismic',\n",
    "                       center=0,\n",
    "                       #vmin=-.03, vmax=.03,\n",
    "                       vmin=-.1, vmax=.1,\n",
    "              \n",
    "                       linewidths=0, xticklabels=False, yticklabels=False,\n",
    "                       #**kws\n",
    "                      )\n",
    "\n",
    "    g.cax.set_position([.09, .17, .03, .45])\n",
    "\n",
    "    ax_col_colors = g.ax_col_colors\n",
    "    box = ax_col_colors.get_position()\n",
    "    box_heatmap = g.ax_heatmap.get_position()\n",
    "    ax_col_colors.set_position([.238, -.01, box.width*1, .025])\n",
    "    \n",
    "    g.fig.suptitle(title+' Distance Comparison', weight='bold')\n",
    "    #g.savefig(sub_class_list[i].plot_path+'dist_comps_mat.png')\n",
    "    \n",
    "    #if filepath is not None:\n",
    "    #    g.savefig('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/leiden/analysis/gradients/figures/op_dist_comps/'+filepath+'.png')\n",
    "    \n",
    "    #return(change_dist_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dist_change(sub_class_list[i].clear_grads, j, range(360), sub_class_list[i].colors, 'Clear-Main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sub_class_list)):\n",
    "    for j,k in zip([sub_class_list[i].clear_main_dist, sub_class_list[i].clear_replace_dist,\n",
    "                    sub_class_list[i].clear_suppress_dist, sub_class_list[i].suppress_main_dist, \n",
    "                    sub_class_list[i].suppress_replace_dist, sub_class_list[i].replace_main_dist],\n",
    "                   ['Clear-Main','Clear-Replace', 'Clear-Suppress', 'Suppress-Main', 'Suppress-Replace', 'Replace-Main']):\n",
    "        dist_change(sub_class_list[i].clear_grads, j, range(360), sub_class_list[i].colors, k)\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_mds(op_df, grad_df, filepath=None): \n",
    "    \n",
    "    main_cor = 1-np.corrcoef(op_df)/2\n",
    "    mds = MDS(n_components=3, random_state=0,n_jobs=4, dissimilarity=\"precomputed\")\n",
    "    X1 = mds.fit_transform(np.array(main_cor))\n",
    "    X1_frame = pd.DataFrame(X1)\n",
    "\n",
    "    X1_frame['grad'] = grad_df['grad']\n",
    "\n",
    "    means = np.array(X1_frame.groupby('grad').mean().reset_index().iloc[:, 1:4])\n",
    "    sds = np.array(X1_frame.groupby('grad').std().reset_index().iloc[:, 1:4])\n",
    "\n",
    "    jets = get_colors('jet_r', 20)\n",
    "    fig = plt.figure()\n",
    "    data = go.Scatter3d(x=means[:,0], y=means[:,1], z=means[:,2], \n",
    "                        mode='markers',\n",
    "                        marker=dict(size=10,\n",
    "                                    #color=newX[:,1],\n",
    "                                    color=jets,\n",
    "                                    opacity=0.7,\n",
    "                                    colorscale=jets)\n",
    "                       )\n",
    "\n",
    "\n",
    "    layout = go.Layout(title_text=\"\",title_x=0.5,title_y=0.8,title_font_size=12)\n",
    "    fig = go.Figure(data=[data], layout=layout)\n",
    "    fig.update_layout(showlegend=False)\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(showticklabels=False)\n",
    "    fig.update_layout(scene = dict(\n",
    "                    xaxis = dict(title= '', ticks= '', showticklabels= False,),\n",
    "                    yaxis = dict(title= '', ticks= '', showticklabels= False,),\n",
    "                    zaxis = dict(title= '', ticks= '', showticklabels= False,),\n",
    "                    ))\n",
    "    fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    if filepath is not None:\n",
    "        fig.write_html(filepath+'_grad_mds.html')\n",
    "        \n",
    "    plt.clf()\n",
    "\n",
    "    \n",
    "#def plot_grad_mds(op_df, grad_num, colors, filepath=None):\n",
    "#    for i in g1_new_orders, g2_new_orders, g3_new_orders:\n",
    "#        testg = i[grad_num].sort_values('index').reset_index(drop=True)\n",
    "#        grad_mds(op_df, testg, colors, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_grad_mds(data, op):\n",
    "    \n",
    "    if op == 'main':\n",
    "        op_df = data.main\n",
    "    elif op == 'replace':\n",
    "        op_df = data.replace\n",
    "    elif op == 'suppress':\n",
    "        op_df = data.suppress\n",
    "    elif op == 'clear':\n",
    "        op_df = data.clear\n",
    "        \n",
    "    x1= data.grad1_all_ops.query('ops == '+'\"'+op+'\"'+'').sort_values('index').reset_index(drop=True)\n",
    "    x2 = data.grad2_all_ops.query('ops == '+'\"'+op+'\"'+'').sort_values('index').reset_index(drop=True)\n",
    "    x3 = data.grad3_all_ops.query('ops == '+'\"'+op+'\"'+'').sort_values('index').reset_index(drop=True)\n",
    "    \n",
    "    for i in x1,x2,x3:\n",
    "        grad_mds(op_df, i, data.plot_path+op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in ['main', 'replace', 'suppress', 'clear']:\n",
    "    *map(partial(plot_class_grad_mds, op=j), sub_class_list),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sub_class_list)):\n",
    "    sub_class_list[i].main_op_disp['sub'] = sub_class_list[i].name\n",
    "    sub_class_list[i].replace_op_disp['sub'] = sub_class_list[i].name\n",
    "    sub_class_list[i].suppress_op_disp['sub'] = sub_class_list[i].name\n",
    "    sub_class_list[i].clear_op_disp['sub'] = sub_class_list[i].name\n",
    "    \n",
    "    sub_class_list[i].main_op_disp.to_csv(sub_class_list[i].plot_path+'main_network_disp.csv')\n",
    "    sub_class_list[i].replace_op_disp.to_csv(sub_class_list[i].plot_path+'replace_network_disp.csv')\n",
    "    sub_class_list[i].suppress_op_disp.to_csv(sub_class_list[i].plot_path+'suppress_network_disp.csv')\n",
    "    sub_class_list[i].clear_op_disp.to_csv(sub_class_list[i].plot_path+'clear_network_disp.csv')\n",
    "    \n",
    "    sub_class_list[i].g1_all['sub'] = sub_class_list[i].name\n",
    "    sub_class_list[i].g2_all['sub'] = sub_class_list[i].name\n",
    "    sub_class_list[i].g3_all['sub'] = sub_class_list[i].name\n",
    "    \n",
    "    sub_class_list[i].g1_all.to_csv(sub_class_list[i].plot_path+'g1_all_ops.csv')\n",
    "    sub_class_list[i].g2_all.to_csv(sub_class_list[i].plot_path+'g2_all_ops.csv')\n",
    "    sub_class_list[i].g3_all.to_csv(sub_class_list[i].plot_path+'g3_all_ops.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_accuracy = pd.concat(accuracy_list)\n",
    "subj_accuracy['subj'] = sub_idx\n",
    "subj_accuracy.to_csv('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/subj/predictions/classifiers/accuracy.csv')\n",
    "\n",
    "subj_evidence = pd.concat(evidence_list)\n",
    "subj_evidence['subj'] = sub_idx\n",
    "subj_evidence.to_csv('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/subj/predictions/classifiers/evidence.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_subs = sub_class_list.copy()\n",
    "\n",
    "main_cent_prediction = []\n",
    "replace_cent_prediction = []\n",
    "suppress_cent_prediction = []\n",
    "clear_cent_prediction = []\n",
    "\n",
    "main_dist_prediction = []\n",
    "replace_dist_prediction = []\n",
    "suppress_dist_prediction = []\n",
    "clear_dist_prediction = []\n",
    "\n",
    "for i in range(len(copy_subs)):\n",
    "    \n",
    "    copy_subs[i].main_centrality_predictions = copy_subs[i].main_centrality.T\n",
    "    copy_subs[i].main_centrality_predictions[['t1_maintain_accuracy']] = accuracy_list[i]['t1_maintain'][0]\n",
    "    copy_subs[i].main_centrality_predictions[['t1_maintain_evidence']] = evidence_list[i]['t1_maintain'][0]\n",
    "    copy_subs[i].main_centrality_predictions[['ID']] = str(copy_subs[i].name).replace('_sm_vector', \"\")\n",
    "        \n",
    "    copy_subs[i].replace_centrality_predictions = copy_subs[i].replace_centrality.T\n",
    "    copy_subs[i].replace_centrality_predictions[['t2_repCat_accuracy']] = accuracy_list[i]['t2_repCat'][0]\n",
    "    copy_subs[i].replace_centrality_predictions[['t2_repCat_evidence']] = evidence_list[i]['t2_repCat'][0]\n",
    "    copy_subs[i].replace_centrality_predictions[['ID']] = str(copy_subs[i].name).replace('_sm_vector', \"\")\n",
    "    \n",
    "    copy_subs[i].suppress_centrality_predictions = copy_subs[i].suppress_centrality.T\n",
    "    copy_subs[i].suppress_centrality_predictions[['t3_target_accuracy']] = accuracy_list[i]['t3_target'][0]\n",
    "    copy_subs[i].suppress_centrality_predictions[['t3_target_evidence']] = evidence_list[i]['t3_target'][0]\n",
    "    copy_subs[i].suppress_centrality_predictions[['ID']] = str(copy_subs[i].name).replace('_sm_vector', \"\")\n",
    "    \n",
    "    copy_subs[i].clear_centrality_predictions = copy_subs[i].clear_centrality.T\n",
    "    copy_subs[i].clear_centrality_predictions[['t4_global_accuracy']] = accuracy_list[i]['t4_global'][0]\n",
    "    copy_subs[i].clear_centrality_predictions[['t4_global_evidence']] = evidence_list[i]['t4_global'][0]\n",
    "    copy_subs[i].clear_centrality_predictions[['ID']] = str(copy_subs[i].name).replace('_sm_vector', \"\")\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------\n",
    "    \n",
    "    copy_subs[i].main_dist_predictions = copy_subs[i].main_dist_df.T\n",
    "    copy_subs[i].main_dist_predictions[['t1_maintain_accuracy']] = accuracy_list[i]['t1_maintain'][0]\n",
    "    copy_subs[i].main_dist_predictions[['t1_maintain_evidence']] = evidence_list[i]['t1_maintain'][0]\n",
    "    copy_subs[i].main_dist_predictions[['ID']] = str(copy_subs[i].name).replace('_sm_vector', \"\")\n",
    "    \n",
    "    \n",
    "    copy_subs[i].replace_dist_predictions = copy_subs[i].replace_dist_df.T\n",
    "    copy_subs[i].replace_dist_predictions[['t2_repCat_accuracy']] = accuracy_list[i]['t2_repCat'][0]\n",
    "    copy_subs[i].replace_dist_predictions[['t2_repCat_evidence']] = evidence_list[i]['t2_repCat'][0]\n",
    "    copy_subs[i].replace_dist_predictions[['ID']] = str(copy_subs[i].name).replace('_sm_vector', \"\")\n",
    "    \n",
    "    \n",
    "    copy_subs[i].suppress_dist_predictions = copy_subs[i].suppress_dist_df.T\n",
    "    copy_subs[i].suppress_dist_predictions[['t3_target_accuracy']] = accuracy_list[i]['t3_target'][0]\n",
    "    copy_subs[i].suppress_dist_predictions[['t3_target_evidence']] = evidence_list[i]['t3_target'][0]\n",
    "    copy_subs[i].suppress_dist_predictions[['ID']] = str(copy_subs[i].name).replace('_sm_vector', \"\")\n",
    "    \n",
    "    \n",
    "    copy_subs[i].clear_dist_predictions = copy_subs[i].clear_dist_df.T\n",
    "    copy_subs[i].clear_dist_predictions[['t4_global_accuracy']] = accuracy_list[i]['t4_global'][0]\n",
    "    copy_subs[i].clear_dist_predictions[['t4_global_evidence']] = evidence_list[i]['t4_global'][0]\n",
    "    copy_subs[i].clear_dist_predictions[['ID']] = str(copy_subs[i].name).replace('_sm_vector', \"\")\n",
    "    \n",
    "    \n",
    "    #----------------------------------------------------------------------------------------\n",
    "    \n",
    "    main_cent_prediction.append(copy_subs[i].main_centrality_predictions)\n",
    "    replace_cent_prediction.append(copy_subs[i].replace_centrality_predictions)\n",
    "    suppress_cent_prediction.append(copy_subs[i].suppress_centrality_predictions)\n",
    "    clear_cent_prediction.append(copy_subs[i].clear_centrality_predictions)\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------\n",
    "        \n",
    "    main_dist_prediction.append(copy_subs[i].main_dist_predictions)\n",
    "    replace_dist_prediction.append(copy_subs[i].replace_dist_predictions)\n",
    "    suppress_dist_prediction.append(copy_subs[i].suppress_dist_predictions)\n",
    "    clear_dist_prediction.append(copy_subs[i].clear_dist_predictions)\n",
    "    \n",
    "     #----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_cent_prediction = pd.concat(main_cent_prediction)\n",
    "replace_cent_prediction = pd.concat(replace_cent_prediction)\n",
    "suppress_cent_prediction = pd.concat(suppress_cent_prediction)\n",
    "clear_cent_prediction = pd.concat(clear_cent_prediction)\n",
    "\n",
    "main_dist_prediction = pd.concat(main_dist_prediction)\n",
    "replace_dist_prediction = pd.concat(replace_dist_prediction)\n",
    "suppress_dist_prediction = pd.concat(suppress_dist_prediction)\n",
    "clear_dist_prediction = pd.concat(clear_dist_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_cent_path = '/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/subj/predictions/centrality/'\n",
    "save_dist_path = '/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/subj/predictions/distance/'\n",
    "\n",
    "for i,j in zip([main_cent_prediction, replace_cent_prediction, suppress_cent_prediction, clear_cent_prediction],\n",
    "              ['main_cent_prediction', 'replace_cent_prediction', 'suppress_cent_prediction', 'clear_cent_prediction']):\n",
    "              i.to_csv(save_cent_path+j+'.csv')\n",
    "for i,j in zip([main_dist_prediction, replace_dist_prediction, suppress_dist_prediction, clear_dist_prediction],\n",
    "               ['main_dist_prediction', 'replace_dist_prediction', 'suppress_dist_prediction', 'clear_dist_prediction']):\n",
    "               i.to_csv(save_dist_path+j+'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script create_ind_gradients.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
