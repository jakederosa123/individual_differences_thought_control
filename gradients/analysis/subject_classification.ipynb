{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/pl/active/banich/studies/Relevantstudies/abcd/data/clustering/analysis/')\n",
    "sys.path.append('/pl/active/banich/studies/Relevantstudies/abcd/env/lib/python3.7/site-packages')\n",
    "\n",
    "from functions import *\n",
    "import shap\n",
    "import xgboost\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from matplotlib import colors as plt_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_main = pd.read_csv('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/grp/main_grads.csv').iloc[:, 1:]\n",
    "group_replace = pd.read_csv('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/grp/replace_grads.csv').iloc[:, 1:]\n",
    "group_suppress = pd.read_csv('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/grp/suppress_grads.csv').iloc[:, 1:]\n",
    "group_clear = pd.read_csv('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/grp/clear_grads.csv').iloc[:, 1:]\n",
    "\n",
    "group_main.columns = ['maintain_g1', 'maintain_g2', 'maintain_g3']\n",
    "group_replace.columns = ['replace_g1', 'replace_g2', 'replace_g3']\n",
    "group_suppress.columns = ['suppress_g1', 'suppress_g2', 'suppress_g3']\n",
    "group_clear.columns = ['clear_g1', 'clear_g2', 'clear_g3']\n",
    "\n",
    "group_grads = pd.concat([group_main, group_replace, group_suppress, group_clear], axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define the pattern to match files\n",
    "pattern = '/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/subj/sub*_sm_vector/sub*_sm_vector_*_all_ops.csv'\n",
    "\n",
    "# Use glob to find all files that match the pattern\n",
    "file_list = sorted(glob.glob(pattern))\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "for file in file_list:\n",
    "    # Extract subject ID from the file path\n",
    "    subject_id = re.search(r'sub(\\d+)_', file).group(1)\n",
    "    \n",
    "    # Extract the part of the filename before '_all_ops'\n",
    "    grad = re.search(r'_(g\\d+)_all_ops', file).group(1)\n",
    "    \n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Add the subject ID as a new column\n",
    "    df['SubID'] = subject_id\n",
    "    \n",
    "    # Add the 'grad' as a new column\n",
    "    df['grad'] = grad\n",
    "    \n",
    "    # Append the DataFrame to the list\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all DataFrames into one, if necessary\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Now 'combined_df' contains all data with a 'SubID' and 'grad' column indicating the subject ID and the gradient respectively.\n",
    "# Adjusting the DataFrame by dropping and reordering columns as required\n",
    "combined_df = combined_df.drop(['Unnamed: 0', 'sub'], axis=1, errors='ignore')  # errors='ignore' handles cases where these columns might not exist\n",
    "combined_df = combined_df[['index', 'SubID', 'maintain', 'replace', 'suppress', 'clear', 'grad']]\n",
    "\n",
    "# combined_df now includes the 'grad' column with the part of the filename you were interested in.\n",
    "\n",
    "combined_df_g1 = combined_df.query('grad == \"g1\"').drop('grad', axis=1).reset_index(drop=True)\n",
    "combined_df_g1.columns = ['index', 'SubID', 'maintain_g1', 'replace_g1', 'suppress_g1', 'clear_g1']\n",
    "\n",
    "combined_df_g2 = combined_df.query('grad == \"g2\"').drop('grad', axis=1).reset_index(drop=True)\n",
    "combined_df_g2.columns = ['index', 'SubID', 'maintain_g2', 'replace_g2', 'suppress_g2', 'clear_g2']\n",
    "\n",
    "combined_df_g3 = combined_df.query('grad == \"g3\"').drop('grad', axis=1).reset_index(drop=True)\n",
    "combined_df_g3.columns = ['index', 'SubID', 'maintain_g3', 'replace_g3', 'suppress_g3', 'clear_g3']\n",
    "\n",
    "combined_all = pd.concat([combined_df_g1, \n",
    "                          combined_df_g2.drop(['index', 'SubID'], axis=1), \n",
    "                          combined_df_g3.drop(['index', 'SubID'], axis=1)], axis=1)\n",
    "\n",
    "combined_all = combined_all[['index', 'SubID', 'maintain_g1', 'maintain_g2', 'maintain_g3', 'replace_g1', 'replace_g2', 'replace_g3', \n",
    "              'suppress_g1', 'suppress_g2', 'suppress_g3', 'clear_g1', 'clear_g2', 'clear_g3']]\n",
    "\n",
    "wm_networks = pd.read_csv('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/rest/rest_gradients/wm_networks.csv')\n",
    "wm_networks = wm_networks.reset_index()\n",
    "\n",
    "combined_all = pd.merge(wm_networks, combined_all, on='index').sort_values(['SubID', 'index'])\n",
    "\n",
    "group_grads = pd.merge(wm_networks, group_grads, on='index')#.sort_values(['SubID', 'index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/pl/active/banich/studies/Relevantstudies/abcd/data/clustering/analysis/')\n",
    "sys.path.append('/pl/active/banich/studies/Relevantstudies/abcd/env/lib/python3.7/site-packages')\n",
    "\n",
    "from functions import *\n",
    "import shap\n",
    "import xgboost\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from matplotlib import colors as plt_colors\n",
    "\n",
    "classes = ['1', '2', '3', '4']\n",
    "\n",
    "class_names = list(group_grads.Subtype.unique())\n",
    "\n",
    "def classy(sub, op):\n",
    "    \n",
    "    global group_grads, combined_all\n",
    "    \n",
    "    X_train = group_grads.filter(regex=op)\n",
    "    X_test = group_grads[['Subtype']]\n",
    "    y_train = combined_all.query('SubID == @sub').filter(regex=op)\n",
    "    y_test = combined_all.query('SubID == @sub')[['Subtype']]\n",
    "\n",
    "    from scipy import stats\n",
    "    xcols = X_train.columns.to_list()\n",
    "    #X_train = pd.DataFrame(stats.zscore(X_train), columns = xcols)\n",
    "    #y_train = pd.DataFrame(stats.zscore(y_train), columns = xcols)\n",
    "\n",
    "    cls = xgboost.XGBClassifier(objective='multi:softmax')\n",
    "\n",
    "    cls.fit(X_train, X_test)\n",
    "\n",
    "    predictions = cls.predict(y_train)\n",
    "    report = pd.DataFrame(classification_report(y_test, predictions, output_dict=True)).iloc[:3, :]\n",
    "    report.columns = ['VN', 'SMN', 'FPCN', 'DMN', 'accuracy', 'macro avg', 'weighted avg']\n",
    "    report = report.assign(SubID = sub)\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maintain_reports = []\n",
    "for i in combined_all.SubID.unique():\n",
    "    maintain_reports.append(classy(i, 'maintain'))\n",
    "    \n",
    "replace_reports = []\n",
    "for i in combined_all.SubID.unique():\n",
    "    replace_reports.append(classy(i, 'replace'))\n",
    "    \n",
    "suppress_reports = []\n",
    "for i in combined_all.SubID.unique():\n",
    "    suppress_reports.append(classy(i, 'suppress'))\n",
    "    \n",
    "clear_reports = []\n",
    "for i in combined_all.SubID.unique():\n",
    "    clear_reports.append(classy(i, 'clear'))\n",
    "    \n",
    "reports = []\n",
    "for i in combined_all.SubID.unique():\n",
    "    reports.append(classy(i, 'g'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_reports(report_list, op=None):\n",
    "    \n",
    "    report_df = pd.concat(report_list).reset_index().sort_values(['index', 'SubID'])\n",
    "    report_df = (report_df[['SubID', 'index', 'VN', 'SMN', 'FPCN', 'DMN', 'accuracy']]\n",
    "                 .rename({'index':'metrics'}, axis=1))\n",
    "\n",
    "    report_f1 = report_df.query('metrics == \"f1-score\"').drop(['metrics', 'accuracy'], axis=1).reset_index(drop=True)\n",
    "    report_f1.columns = ['SubID', 'VN_f1', 'SMN_f1', 'FPCN_f1', 'DMN_f1']\n",
    "\n",
    "    report_precision = report_df.query('metrics == \"precision\"').drop(['metrics', 'accuracy'], axis=1).reset_index(drop=True)\n",
    "    report_precision.columns = ['SubID', 'VN_precision', 'SMN_precision', 'FPCN_precision', 'DMN_precision']\n",
    "\n",
    "    report_recall = report_df.query('metrics == \"f1-score\"').drop(['metrics', 'accuracy'], axis=1).reset_index(drop=True)\n",
    "    report_recall.columns = ['SubID', 'VN_recall', 'SMN_recall', 'FPCN_recall', 'DMN_recall']\n",
    "\n",
    "    report_accuracy = report_df[['SubID', 'accuracy']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    report_final = pd.merge(report_f1, report_precision, on ='SubID')\n",
    "    #report_final = pd.merge(report_recall, report_final, on ='SubID')\n",
    "    report_final = pd.merge(report_accuracy, report_final, on ='SubID')\n",
    "\n",
    "    report_final[['SubID']] = report_final[['SubID']].astype(int)\n",
    "    \n",
    "    if op is not None:\n",
    "        report_final.columns = ['SubID'] + [col + '_'+op for col in report_final.iloc[:, 1:].columns.to_list()]\n",
    "\n",
    "    return report_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maintain_report = clean_reports(maintain_reports, 'maintain')\n",
    "replace_report = clean_reports(maintain_reports, 'replace')\n",
    "suppress_report = clean_reports(maintain_reports, 'suppress')\n",
    "clear_report = clean_reports(maintain_reports, 'clear')\n",
    "\n",
    "report_df = clean_reports(reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_report_df = pd.merge(maintain_report, replace_report, on ='SubID')\n",
    "all_report_df = pd.merge(all_report_df, suppress_report, on ='SubID')\n",
    "all_report_df = pd.merge(all_report_df, clear_report, on ='SubID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_subid = pd.read_csv('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/subj/matched_subid.csv')\n",
    "matched_subid.columns = ['sub', 'SubID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_final = pd.merge(matched_subid, report_df, on='SubID').drop('SubID', axis=1).rename({'sub':'SubID'}, axis=1)\n",
    "\n",
    "all_report_final = pd.merge(matched_subid, all_report_df, on='SubID').drop('SubID', axis=1).rename({'sub':'SubID'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_data = pd.read_csv('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/analysis/ClearMem_Z_Average.csv')\n",
    "z_data = z_data[['SubID', 'z_ave', 'PSWQ_total', 'WBSI_total', 'RRS_total', 'RRS_depression', 'RRS_brooding', 'RRS_reflection']]\n",
    "z_data = z_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_z = pd.merge(z_data, report_final, on='SubID')\n",
    "all_report_z = pd.merge(z_data, all_report_final, on='SubID')\n",
    "\n",
    "report_z = report_z.query('SubID != 76')\n",
    "all_report_z = all_report_z.query('SubID != 76')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pls(data, target, networks, color, save=None):\n",
    "\n",
    "\n",
    "    regex_string = '|'.join(networks)\n",
    "\n",
    "    import seaborn as sns\n",
    "    from sklearn.cross_decomposition import PLSRegression\n",
    "    from scipy.stats import t\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.model_selection import LeaveOneOut\n",
    "    # Create an instance of LeaveOneOut\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    \n",
    "    scaler=MinMaxScaler()\n",
    "    \n",
    "    X_data = data\n",
    "    # Extract the feature matrix X and target variable y\n",
    "    X = (X_data \n",
    "             .filter(regex=regex_string)\n",
    "             #.filter(like='dispersion_md')\n",
    "             #.filter(regex='^(?!.*(main|suppress|replace)).*$')\n",
    "             #.filter(regex='^(?!.*(maintain_smn)).*$') \n",
    "            )\n",
    "    #print(X.columns)\n",
    "\n",
    "    # fit and transform the entire dataframe\n",
    "    #X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    y = X_data[target]\n",
    "    y = pd.DataFrame(scaler.fit_transform(y), columns=y.columns)\n",
    "\n",
    "    # Set the number of permutations and bootstrap repetitions\n",
    "    n_permutations = 10000\n",
    "    n_bootstrap = 10000\n",
    "\n",
    "    pls = PLSRegression(n_components=1)\n",
    "        \n",
    "    # Perform leave-one-out cross-validation to determine the optimal number of components\n",
    "    #mse_values = []\n",
    "    #for n_components in range(1, len(X.columns)+1):\n",
    "    #    pls.n_components = n_components\n",
    "    #    y_pred = cross_val_predict(pls, X, y, cv=loo)\n",
    "    #    mse = mean_squared_error(y, y_pred)\n",
    "    #    mse_values.append(mse)\n",
    "\n",
    "    #pls = PLSRegression(n_components=1)\n",
    "    #optimal_n_components = np.argmin(mse_values) + 1\n",
    "    #print('Optimal number of components:', optimal_n_components)\n",
    "\n",
    "    # Fit the PLSRegression model to the feature matrix X and target variable y using the optimal number of components\n",
    "    pls.n_components = 1 #optimal_n_components\n",
    "    pls.fit(X, y)\n",
    "\n",
    "    # Compute the singular value of the fitted model\n",
    "    x_score = pls.x_scores_\n",
    "    y_score = pls.y_scores_\n",
    "    singular_value = np.dot(x_score.T, y_score)\n",
    "    print('Starting Permutations')\n",
    "    # Generate permuted data matrices by randomly reordering the rows of the original data matrix X\n",
    "    permuted_singular_values = []\n",
    "    for i in range(n_permutations):\n",
    "        permuted_X = X.sample(frac=1, replace=False)\n",
    "        pls.fit(permuted_X, y)\n",
    "        x_score = pls.x_scores_\n",
    "        y_score = pls.y_scores_\n",
    "        permuted_singular_value = np.dot(x_score.T, y_score)\n",
    "        permuted_singular_values.append(permuted_singular_value)\n",
    "\n",
    "    # Compute the p-value of the singular value by comparing it to the null distribution of singular values\n",
    "    p_value = (np.sum(permuted_singular_values >= singular_value) + 1) / (n_permutations + 1)\n",
    "    \n",
    "    import seaborn as sns\n",
    "    from sklearn.cross_decomposition import PLSRegression\n",
    "    from scipy.stats import t\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import statsmodels.api as sm\n",
    "    import matplotlib\n",
    "\n",
    "       # Prepare data for the histogram\n",
    "    singular_list = [i[0][0] for i in permuted_singular_values]\n",
    "    singulars = pd.DataFrame(singular_list, columns=['Singulars'])\n",
    "    singular_val = singular_value[0][0]\n",
    "\n",
    "    # Begin plotting\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))  # Set up a figure with two subplots\n",
    "    \n",
    "    # Scatter plot on the left\n",
    "    sns.set_theme(style=\"white\")\n",
    "\n",
    "    sns.regplot(x=x_score.flatten(), y=y_score.flatten(), \n",
    "                ax=axs[0], color=color, scatter_kws={'alpha':0.6}, truncate=False)\n",
    "    axs[0].set_xlabel('Network Dispersion (Latent Variable)', fontsize=14)\n",
    "    axs[0].set_ylabel('Behavior (Latent Variable)', fontsize=14)\n",
    "    axs[0].set_title('Network Dispersion Predicting Behavior', fontsize=14, fontweight='bold')\n",
    "    \n",
    "       # Displaying 'permuted P = ' with manual positioning for readability\n",
    "    axs[0].text(.06, 1, 'permuted ', transform=axs[0].transAxes, fontsize=13, verticalalignment='top', color='black')\n",
    "    axs[0].text(.22, 1, 'P', transform=axs[0].transAxes, fontsize=13, verticalalignment='top', color='black', fontstyle='italic')\n",
    "    axs[0].text(.24, 1, ' = {:.3f}'.format(p_value), transform=axs[0].transAxes, fontsize=13, verticalalignment='top', color='black')\n",
    "    \n",
    "    sns.despine(ax=axs[0])\n",
    "\n",
    "    # Histogram on the right\n",
    "    sns.histplot(singulars, kde=False, ax=axs[1])\n",
    "    axs[1].axvline(x=singular_val, color='r', linestyle='--', label='Singular Value')\n",
    "    axs[1].legend()\n",
    "    axs[1].set_title(\"Singular Value vs. Null Distribution\", fontsize=14, fontweight='bold')\n",
    "    axs[1].set_xlabel('Permuted Singular Values (Null)', fontsize=14)\n",
    "    sns.despine(ax=axs[1])\n",
    "    plt.tight_layout()  # Adjust layout to not overlap\n",
    "    \n",
    "    if save is not None:\n",
    "        save_path = f\"/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/analysis/pls_plots/{save}.png\"\n",
    "        plt.savefig(save_path, dpi=700)\n",
    "        \n",
    "    plt.tight_layout()  # Adjust layout to not overlap\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    return p_value, x_score, y_score, permuted_singular_values, singular_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [['RRS_total', 'PSWQ_total', 'WBSI_total'], ['z_ave']]\n",
    "colors = ['#64b3a3', '#b19cd9']\n",
    "networks = ['accuracy','VN', 'SMN', 'FPCN', 'DMN']\n",
    "names = ['rrs_pws_wbsi_classacc', 'z_ave_classacc']\n",
    "for i,j,k in zip(targets, colors, names):\n",
    "    p_value, x_score, y_score, permuted_singular_values, singular_value = pls(report_z, i, networks, j, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [['RRS_total', 'PSWQ_total', 'WBSI_total'], ['z_ave']]\n",
    "colors = ['#64b3a3', '#b19cd9']\n",
    "networks = ['accuracy','VN', 'SMN', 'FPCN', 'DMN']\n",
    "\n",
    "names = ['rrs_pws_wbsi_allops_classacc', 'z_ave_allops_classacc']\n",
    "for i,j,k in zip(targets, colors, names):\n",
    "    p_value, x_score, y_score, permuted_singular_values, singular_value = pls(all_report_z, i, networks, j, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
