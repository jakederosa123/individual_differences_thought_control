{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('/pl/active/banich/studies/Relevantstudies/abcd/env/lib/python3.7/site-packages')\n",
    "sys.path.append('/pl/active/banich/studies/Clearvale/jake_scripts/Amy_flywheel_scripts/')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# adding Folder_2 to the system path\n",
    "sys.path.insert(0, '/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/leiden/analysis/')\n",
    "from clearmem_my_functions import *\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/jade6100/.local/lib/python3.7/site-packages')\n",
    "import scikit_posthocs as sp\n",
    "import itertools\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "import sys \n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "from sklearn.manifold import MDS\n",
    "import scipy.spatial.distance as sp_distance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import animation\n",
    "\n",
    "#import statsmodels.api as sm\n",
    "#from statsmodels.formula.api import ols\n",
    "#from scipy import stats\n",
    "#import scikit_posthocs as sp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import manifold\n",
    "\n",
    "def getF(data, var, group):\n",
    "    data[var] = data[var].astype('float')\n",
    "    model = ols(var + '~ C('+group+')', data=data).fit()\n",
    "    anova_table = np.array(sm.stats.anova_lm(model, type=2)[['F', 'PR(>F)']])[0]\n",
    "    return anova_table.round(4)\n",
    "\n",
    "def getposthoc(data, var, group):\n",
    "    data[var] = data[var].astype('float')\n",
    "    model = ols(var + '~ C('+group+')', data=data).fit()\n",
    "    post_hoc = sp.posthoc_ttest(data, val_col=var, group_col=group,\n",
    "                                p_adjust='fdr_bh').sort_index().sort_index(axis = 1)\n",
    "    post_hoc.columns = list(range(1, len(uni(data[group]))+1))\n",
    "    return post_hoc\n",
    "\n",
    "def vsim(data, parcel=None):\n",
    "    \n",
    "    if parcel is None:\n",
    "        parcel_row = pd.DataFrame(np.flip(np.array(data.T))).T\n",
    "        \n",
    "    else:\n",
    "        parcel_row = pd.DataFrame(np.flip(np.array(data.iloc[[parcel]].T))).T\n",
    "    a = np.array(parcel_row.T.iloc[:, 0: 1]).squeeze()\n",
    "    n = int(np.sqrt(len(a)*2))+1\n",
    "    mask = np.tri(n,dtype=bool, k=-1) # or np.arange(n)[:,None] > np.arange(n)\n",
    "    out = np.zeros((n,n))\n",
    "    out[mask] = a\n",
    "    out = out + out.T - np.diag(np.diag(out))\n",
    "    out[np.diag_indices_from(out)]\n",
    "    sim_mat = pd.DataFrame(np.flip(out))\n",
    "  \n",
    "\n",
    "    return(sim_mat)\n",
    "    \n",
    "def grad_centroid_vsim(data, grad): \n",
    "    new_row_cluster = data[data['grad'] == grad]\n",
    "    new_row_cluster = new_row_cluster.drop(['grad'], axis=1)\n",
    "    clust_centroid = pd.DataFrame(new_row_cluster.mean(), columns = ['Mean']).T\n",
    "    \n",
    "    sim_mat = vsim(clust_centroid)\n",
    "    \n",
    "    return(sim_mat)\n",
    "\n",
    "\n",
    "def threshold_proportional(W,p,copy=True):\n",
    "\n",
    "    if p>1 or p<0:\n",
    "        raise BCTParamError('Threshold must be in range [0,1]')\n",
    "    if copy: W=W.copy()\n",
    "    n=len(W)\t\t\t\t\t\t# number of nodes\n",
    "    np.fill_diagonal(W, 0)\t\t\t# clear diagonal\n",
    "\n",
    "    if np.all(W==W.T):\t\t\t\t# if symmetric matrix\n",
    "        W[np.tril_indices(n)]=0\t\t# ensure symmetry is preserved\n",
    "        ud=2\t\t\t\t\t\t# halve number of removed links\n",
    "    else:\n",
    "        ud=1\n",
    "\n",
    "    ind=np.where(W)\t\t\t\t\t# find all links\n",
    "\n",
    "    I=np.argsort(W[ind])[::-1]\t\t# sort indices by magnitude\n",
    "\n",
    "    en=int(round((n*n-n)*p/ud))\t\t# number of links to be preserved\n",
    "\n",
    "    W[(ind[0][I][en:],ind[1][I][en:])]=0\t# apply threshold\n",
    "    #W[np.ix_(ind[0][I][en:], ind[1][I][en:])]=0\n",
    "\n",
    "    if ud==2:\t\t\t\t\t\t# if symmetric matrix\n",
    "        W[:,:]=W+W.T\t\t\t\t\t\t# reconstruct symmetry\n",
    "\n",
    "    return W\n",
    "\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import itertools\n",
    "\n",
    "def plot_3Dfigure(newX, colors, title='', net=None, filepath=None):\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    if net is not None:\n",
    "        data = go.Scatter3d(x=newX[:,0], y=newX[:,1], z=newX[:,2], \n",
    "                            mode='markers',\n",
    "                            marker=dict(size=5,\n",
    "                                        #color=newX[:,1],\n",
    "                                        color=colors,\n",
    "                                        opacity=0.7,\n",
    "                                        colorscale=colors)\n",
    "                           )\n",
    "    else:\n",
    "        data = go.Scatter3d(x=newX[:,0], y=newX[:,1], z=newX[:,2], \n",
    "                            mode='markers',\n",
    "                            marker=dict(size=5,\n",
    "                                        color=newX[:,1],\n",
    "                                        opacity=0.7,\n",
    "                                        colorscale='jet')\n",
    "                           )\n",
    "    \n",
    "    layout = go.Layout(title_text=title,title_x=0.5,title_y=0.8,title_font_size=12)\n",
    "    fig = go.Figure(data=[data], layout=layout)\n",
    "    fig.update_layout(showlegend=False)\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(showticklabels=False)\n",
    "    fig.update_layout(scene = dict(\n",
    "                    xaxis = dict(title= '', ticks= '', showticklabels= False,),\n",
    "                    yaxis = dict(title= '', ticks= '', showticklabels= False,),\n",
    "                    zaxis = dict(title= '', ticks= '', showticklabels= False,),\n",
    "                    ))\n",
    "    fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "    #fig.show()\n",
    "    \n",
    "    if filepath is not None:\n",
    "        fig.write_html(filepath+'3d_grads.html')\n",
    "        \n",
    "\n",
    "from pylab import *\n",
    "\n",
    "def get_color_maps(cmap, n, nrep):\n",
    "    from PIL import Image, ImageColor\n",
    "\n",
    "    cmap = cm.get_cmap(cmap, n)    # PiYG\n",
    "    converted_list = []\n",
    "    for i in range(cmap.N):\n",
    "        rgba = cmap(i)\n",
    "        # rgb2hex accepts rgb or rgba\n",
    "        hexc = matplotlib.colors.rgb2hex(rgba)\n",
    "        codes = ImageColor.getcolor(str(hexc), \"RGB\")\n",
    "\n",
    "        c1 = list(itertools.repeat(int(codes[0])/255, nrep))\n",
    "        c2 = list(itertools.repeat(int(codes[1])/255, nrep))\n",
    "        c3 = list(itertools.repeat(int(codes[2])/255, nrep))\n",
    "        \n",
    "        converted = pd.concat([pd.DataFrame(c1), pd.DataFrame(c2), pd.DataFrame(c3)], axis = 1)\n",
    "        converted.columns = ['r', 'g', 'b']\n",
    "        converted_list.append(converted)\n",
    "        \n",
    "    final = pd.concat(converted_list).reset_index(drop=True)\n",
    "    final['index_new'] = list(range(0,final.shape[0]))\n",
    "    \n",
    "    return final\n",
    "\n",
    "def show_grads(data):\n",
    "    import nibabel as nib\n",
    "    import nilearn.plotting as plotting\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "    import pandas as pd\n",
    "    import hcp_utils as hcp\n",
    "\n",
    "    def listToDict(lstA, lstB):\n",
    "        zipped = zip(lstA, lstB)\n",
    "        op = dict(zipped)\n",
    "        return op\n",
    "\n",
    "    def uni(list1):\n",
    "        unique_list = []\n",
    "        # traverse for all elements\n",
    "        for x in list1:\n",
    "            # check if exists in unique_list or not\n",
    "            if x not in unique_list:\n",
    "                unique_list.append(x)\n",
    "\n",
    "        return(unique_list)\n",
    "\n",
    "    glasser = pd.read_csv('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/leiden/analysis/glasser_regions/spearman_subtype_glasser_regions.csv').drop('regionID', axis =1).rename({'Unnamed: 0':'regionID'}, axis = 1)\n",
    "    glasser['regionID'] = glasser['regionID'] + 1\n",
    "    \n",
    "    mesh_path = '/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/leiden/hcp-utils/hcp_utils/data/'\n",
    "    mesh_sub = hcp.load_surfaces(example_filename=mesh_path+'S1200.R.pial_MSMAll.32k_fs_LR.surf.gii')\n",
    "    \n",
    "    regs = data \n",
    "    regs['regionID'] = regs['index'] + 1\n",
    "    regs = pd.merge(regs, glasser, on = 'regionID')\n",
    "    regs = regs[['regionID', 'Subtype_x', 'grad', 'regionName', 'regionLongName', 'Lobe', 'cortex', 'r', 'g', 'b']]\n",
    "  \n",
    "    maps_df = pd.DataFrame(hcp.mmp.rgba).T\n",
    "    maps_df = maps_df.reset_index().rename({'index':'regionID'}, axis = 1)\n",
    "    maps_df['regionID'] = maps_df['regionID'] +1\n",
    "    maps_df = pd.DataFrame(hcp.mmp.rgba).T\n",
    "    maps_df = maps_df.reset_index().rename({'index':'regionID'}, axis = 1)\n",
    "    maps_df['regionID'] = maps_df['regionID'] +1\n",
    "\n",
    "    new_maps_df = pd.merge(regs, maps_df, on = \"regionID\").sort_values('regionID')\n",
    "    new_maps_df[0] = new_maps_df['r']\n",
    "    new_maps_df[1] = new_maps_df['g']\n",
    "    new_maps_df[2] = new_maps_df['b']\n",
    "    new_maps_df = new_maps_df[[0, 1,2,3]]\n",
    "\n",
    "    first = pd.DataFrame(np.array([0, 0, 0, 0])).T\n",
    "    more = pd.concat([pd.DataFrame(np.zeros(20)), pd.DataFrame(np.zeros(20)), pd.DataFrame(np.zeros(20)), pd.DataFrame(np.zeros(20))], axis= 1)\n",
    "    more.columns = [0,1,2,3]\n",
    "\n",
    "    new_maps_df = pd.concat([first, new_maps_df, more])\n",
    "\n",
    "    map_l = []\n",
    "    out_l = []\n",
    "    for i in range(0, len(new_maps_df)):\n",
    "        map_l.append(i)\n",
    "        out_l.append(np.array(new_maps_df.iloc[i:i+1])[0])\n",
    "\n",
    "    hcp.mmp.rgba = listToDict(map_l, out_l)\n",
    "\n",
    "    return hcp.view_parcellation(mesh_sub.inflated, hcp.mmp)\n",
    "\n",
    "def get_colors(cmap, n):\n",
    "        from PIL import Image, ImageColor\n",
    "\n",
    "        cmap = cm.get_cmap(cmap, n)    # PiYG\n",
    "        converted_list = []\n",
    "        for i in range(cmap.N):\n",
    "            rgba = cmap(i)\n",
    "            # rgb2hex accepts rgb or rgba\n",
    "            hexc = matplotlib.colors.rgb2hex(rgba)\n",
    "            converted_list.append(hexc)\n",
    "\n",
    "        return converted_list\n",
    "    \n",
    "    \n",
    "def show_code(function):  \n",
    "    import inspect\n",
    "    lines = inspect.getsource(function)\n",
    "    print(lines)\n",
    "    \n",
    "jets = get_colors('jet_r', 20)\n",
    "\n",
    "\n",
    "net_cols = ['#FCFF0D', '#21DFB4', '#4E00A2', '#F00087']\n",
    "ops_cols = ['#F0180A', '#F08B0A', '#6DAE45', '#0A5AF0']\n",
    "\n",
    "\n",
    "\n",
    "def reduce_memory_usage(df, verbose=False):\n",
    "    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from glob import glob\n",
    "subj_ids = '/pl/active/banich/studies/wmem/fmri/operation_rsa/subj/*'\n",
    "subj_ids_sorted = sorted(glob(subj_ids, recursive = True))\n",
    "\n",
    "for i in range(0,len(subj_ids_sorted)):\n",
    "    subj_ids_sorted[i] = subj_ids_sorted[i].replace('/pl/active/banich/studies/wmem/fmri/operation_rsa/subj/subj_operation_sub-', '').replace('/', '').replace('.mat', '')\n",
    "    #subj_ids_sorted[i] = 'sub' + subj_ids_sorted[i] + \"-\" + str(i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_func_mat(data):\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    X = np.array(data)\n",
    "    \n",
    "    X_Z = (X - np.mean(X, axis=0, keepdims=True)) / np.std(X, axis=0, dtype=np.float64, ddof=1, keepdims=True)\n",
    "    D, rho = spearmanr(np.array(X_Z), axis=1)\n",
    "\n",
    "    perc = np.array([np.percentile(x, 90) for x in D])\n",
    "\n",
    "    for i in range(D.shape[0]):\n",
    "        D[i, D[i,:] < perc[i]] = 0    \n",
    "    \n",
    "    D[D < 0] = 0\n",
    "    #D = 1 - pairwise_distances(D, metric = 'cosine')\n",
    "    #scaler = MinMaxScaler(feature_range=(.5,1))\n",
    "\n",
    "    #D = scaler.fit_transform(D)\n",
    "    \n",
    "    D = pd.DataFrame(D)\n",
    "    # D = aff\n",
    "    \n",
    "    return(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = pd.read_csv('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/leiden/group_sm_vector_spearman/Output/Results/group_sm_vector_spearman_Full_Subtypes.csv')\n",
    "\n",
    "gmat_main = pd.DataFrame(create_func_mat(group.iloc[:, 5:].iloc[:, 0:10332]))\n",
    "gmat_replace = pd.DataFrame(create_func_mat(group.iloc[:, 5:].iloc[:, 10332:int(10332*2)]))\n",
    "gmat_suppress = pd.DataFrame(create_func_mat(group.iloc[:, 5:].iloc[:, int(10332*2):int(10332*3)]))\n",
    "gmat_clear = pd.DataFrame(create_func_mat(group.iloc[:, 5:].iloc[:, int(10332*3):int(10332*4)]))\n",
    "\n",
    "group_subs = group[['Unnamed: 0', 'Subtype']].rename({'Unnamed: 0':'index'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_path = '/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/leiden/subj_sm_ind/*'\n",
    "from glob import glob\n",
    "\n",
    "sub_idx = glob(glob_path, recursive = True)\n",
    "\n",
    "for i in range(0,len(sub_idx)):\n",
    "    sub_idx[i] = sub_idx[i].replace('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/leiden/subj_sm_ind/', '').replace('/', '')\n",
    "    \n",
    "sub_idx = list(filter(lambda k: 'sub' in k, sub_idx))\n",
    "\n",
    "sub_idx = list(filter(lambda k: '.csv' not in k, sub_idx))\n",
    "\n",
    "#remove sub019 because no accuracy or evdidence classifiers\n",
    "sub_idx = sorted([x for x in sub_idx if 'sub019_sm_vector' not in x])\n",
    "sub_idx = sub_idx[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_mats(data, parcel_index, gfm):\n",
    "    \n",
    "    sfm = pd.DataFrame(create_func_mat(data))\n",
    "    sfm.columns = parcel_index\n",
    "    sfm.index = parcel_index\n",
    "    smf_cols = list(pd.DataFrame(sfm).columns)\n",
    "\n",
    "    not_in_list = [x for x in list(range(360)) if x not in smf_cols]\n",
    "    \n",
    "    sfm_copy = sfm.copy()\n",
    "    for i in not_in_list:\n",
    "        sfm_copy[i] = gfm[i]\n",
    "        \n",
    "    sfm_copy = sfm_copy.T\n",
    "\n",
    "    for i in not_in_list:\n",
    "        sfm_copy[i] = gfm[i]\n",
    "\n",
    "    sfm_copy = pd.DataFrame(sfm_copy.sort_index().sort_index(axis=1))\n",
    "\n",
    "\n",
    "    return(sfm_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mats(data_path, group_subs):\n",
    "    \n",
    "    df1 = pd.read_csv(data_path).drop('Subtype', axis =1)\n",
    "\n",
    "    main_cols = list(map(str,range(0, 10332)))\n",
    "    replace_cols = list(map(str,range(10332, 10332*2)))\n",
    "    suppress_cols = list(map(str,range(10332*2, 10332*3)))\n",
    "    clear_cols = list(map(str,range(10332*3, 10332*4)))\n",
    "    \n",
    "    lh = list(df1[df1['node'].str.contains(\"LH\")]['node'].str.replace(r'\\D', '').astype(int) -1)\n",
    "    rh = list(df1[df1['node'].str.contains(\"RH\")]['node'].str.replace(r'\\D', '').astype(int) + 179)\n",
    "\n",
    "    parcel_index = lh + rh\n",
    "    df1.index = parcel_index\n",
    "    \n",
    "    not_in_list = [x for x in list(range(360)) if x not in parcel_index]\n",
    "    \n",
    "    df1_cols = list(df1.iloc[:, 4:].columns)\n",
    "    \n",
    "    df1['index'] = df1.index\n",
    "    df1 = pd.merge(df1, group_subs, on = 'index')\n",
    "    df1.index = parcel_index\n",
    "    \n",
    "    \n",
    "    if len(parcel_index) < 360:\n",
    "         \n",
    "        ts1_subs = df1[['index', 'Subtype']]\n",
    "        add_subs = group_subs.T[not_in_list].T\n",
    "        copy_frame = df1\n",
    "        ts1_added_subs = pd.concat([ts1_subs, add_subs])\n",
    "        ts1_added_subs = pd.concat([copy_frame.drop(['index', 'Subtype'], axis=1), ts1_added_subs], axis=1)\n",
    "\n",
    "        df1 = ts1_added_subs\n",
    "        \n",
    "\n",
    "    return df1, parcel_index, not_in_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class subj_outputs:\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.df1, self.parcel_index, self.not_in_list = get_mats(self.path, group_subs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.15318344696425\n"
     ]
    }
   ],
   "source": [
    "subj_path = []\n",
    "for i in sub_idx:\n",
    "    subj_path.append('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/leiden/subj_sm_ind/'+i+'/Output/Results/'+i+'_Full_Subtypes.csv')\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "sub_class_list = *map(subj_outputs, subj_path),\n",
    "end = timer()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- processed:sub001_sm_vector\n",
      "----------------------- processed:sub002_sm_vector\n",
      "----------------------- processed:sub003_sm_vector\n",
      "----------------------- processed:sub004_sm_vector\n",
      "----------------------- processed:sub005_sm_vector\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(range(len(sub_class_list)), sub_idx):\n",
    "        print('----------------------- processed:'+j)\n",
    "        os.system(f'mkdir -p /pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/subj/{j}')\n",
    "        sub_class_list[i].name = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path = '/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/gradients/subj/'\n",
    "for i in range(len(sub_class_list)):\n",
    "    sub_class_list[i].plot_path = plot_path + sub_class_list[i].name+'/'+sub_class_list[i].name+'_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub_class_list[0].df1[main_cols]\n",
    "\n",
    "def sub_trial_cor(sub_num):\n",
    "    \n",
    "    global sub_class_list\n",
    "    \n",
    "    def get_parc_cors(sub_class_list, sub_num, op_name):\n",
    "        \n",
    "        if op_name == 'main':\n",
    "            op_list = list(map(str,range(0, 10332)))\n",
    "    \n",
    "        if op_name == 'replace':\n",
    "             op_list = list(map(str,range(10332, 10332*2)))\n",
    "\n",
    "        if op_name == 'suppress':\n",
    "             op_list = list(map(str,range(10332*2, 10332*3)))\n",
    "\n",
    "        if op_name == 'clear':\n",
    "             op_list = list(map(str,range(10332*3, 10332*4)))\n",
    "\n",
    "        \n",
    "        columns_to_select = [col for col in op_list if col in sub_class_list[sub_num].df1.columns]\n",
    "        selected_df = sub_class_list[sub_num].df1[columns_to_select]\n",
    "     \n",
    "        parc_cors =[]\n",
    "        #for i in sub_class_list[sub_num].parcel_index:\n",
    "        for i in range(360):\n",
    "            parc_cors.append(selected_df.iloc[i].mean())\n",
    "             #parc_cors.append(selected_df.query('parcel in @i').mean())\n",
    "    \n",
    "        parc_cors_df = (pd.DataFrame(parc_cors, columns=[f'{op_name}_cors'])\n",
    "                        .assign(sub = sub_class_list[sub_num].name[3:6]).iloc[:, [1,0]]\n",
    "                       )\n",
    "        \n",
    "        return parc_cors_df\n",
    "\n",
    "    sub_op_cors=[]\n",
    "    for i in ['main', 'replace', 'suppress', 'clear']:\n",
    "        sub_op_cors.append(get_parc_cors(sub_class_list, sub_num, i))\n",
    "        \n",
    "    global group\n",
    "    \n",
    "    networks = group[['Key', 'Subtype']]\n",
    "    networks['parcel'] = networks['Key'].astype(int)\n",
    "    networks = networks[['parcel', 'Subtype']]\n",
    "    \n",
    "    mapping = {1: 'VN', 2: 'SMN', 3: 'FPCN', 4: 'DMN'}\n",
    "    networks['Subtype'] = networks['Subtype'].map(mapping)\n",
    "\n",
    "    sub_op_cors_df = pd.concat(sub_op_cors, axis=1).iloc[:, [0,1,3,5,7]].dropna()\n",
    "    sub_op_cors_df['parcel'] = sub_class_list[sub_num].parcel_index \n",
    "    sub_op_cors_df['parcel'] = sub_op_cors_df['parcel'] + 1\n",
    "    \n",
    "    sub_op_cors_df = pd.merge(sub_op_cors_df, networks, on='parcel')\n",
    "    sub_op_cors_df = sub_op_cors_df[['sub', 'parcel', 'Subtype', 'main_cors', 'replace_cors', 'suppress_cors', 'clear_cors']]\n",
    "        \n",
    "        \n",
    "    #sub_name =  sub_op_cors_df['sub'].unique()[0]\n",
    "    #sub_cors_net = sub_op_cors_df.groupby('Subtype').mean().reset_index().drop('parcel', axis=1).melt(id_vars='Subtype')\n",
    "    \n",
    "    #sub_cors_net['net_op_cor'] = sub_cors_net['variable'] + '_' + sub_cors_net['Subtype']\n",
    "    \n",
    "    #sub_cors_net = (sub_cors_net[['net_op_cor', 'value']].reset_index()\n",
    "    #                 .pivot(index='index', columns='net_op_cor', values='value')\n",
    "    #                 .reset_index(drop=True)\n",
    "    #                ).melt().dropna().T\n",
    "    #sub_cors_net.columns = sub_cors_net.iloc[0].to_list()\n",
    "    #sub_cors_net = pd.DataFrame(sub_cors_net.iloc[1]).T.reset_index(drop=True)\n",
    "    \n",
    "    # Calculate the variance by network instead of the mean\n",
    "    sub_cors_net = sub_op_cors_df.groupby('Subtype').var().reset_index().drop('parcel', axis=1).melt(id_vars='Subtype')\n",
    "\n",
    "    # Create the 'net_op_cor' column by combining 'variable' and 'Subtype'\n",
    "    sub_cors_net['net_op_cor'] = sub_cors_net['variable'] + '_' + sub_cors_net['Subtype']\n",
    "\n",
    "    # Reshape the DataFrame to have 'net_op_cor' as columns and 'value' as values\n",
    "    sub_cors_net = (sub_cors_net[['net_op_cor', 'value']].reset_index()\n",
    "                    .pivot(index='index', columns='net_op_cor', values='value')\n",
    "                    .reset_index(drop=True)\n",
    "                   ).melt().dropna().T\n",
    "    sub_cors_net.columns = sub_cors_net.iloc[0].to_list()\n",
    "    sub_cors_net = pd.DataFrame(sub_cors_net.iloc[1]).T.reset_index(drop=True)\n",
    "    sub_cors_net['sub'] = sub_class_list[sub_num].name[3:6]\n",
    "    # good \n",
    "    op_cors = pd.DataFrame(sub_op_cors_df.mean()).T.iloc[:, 2:]\n",
    "    op_cors['sub'] = sub_class_list[sub_num].name[3:6]\n",
    "\n",
    "    #all_cors = op_cors\n",
    "    all_cors = pd.merge(op_cors, sub_cors_net, on='sub')\n",
    "    all_cors= all_cors[['sub'] + [col for col in all_cors.columns if col != 'sub']]\n",
    "\n",
    "    return all_cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sub_cors_df = pd.concat([sub_trial_cor(i) for i in range(len(sub_class_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sub_cors_df.to_csv('/pl/active/banich/studies/wmem/fmri/operation_rsa/grp/bootstrapped_regressions/sub_rsa_mat_cors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook sub_rsa_mat_cors.ipynb to script\n",
      "[NbConvertApp] Writing 20411 bytes to sub_rsa_mat_cors.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script sub_rsa_mat_cors.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
